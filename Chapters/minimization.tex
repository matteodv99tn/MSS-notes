\chapter{Constrained Minimization}

\section*{Calculus revision}
	Given a function $f:\mathds R^n\rightarrow \mathds R$ it's possible to compute it's \textbf{minimum} by doing the assumption that $f$ has a \textbf{Lipschitz continuos gradient}, notated as $f\in C^1(\R^n)$, meaning that
	\[ \exists \, \gamma > 0 \quad \textrm{such that} \quad \left\| \nabla f(\vett x)^t - \nabla f(\vett y)^t\right\| \leq \gamma \| \vett x - \vett y\| \qquad \forall \vett x,\vett y \in \R^n  \]
	
	A point $\vett x^*\in R$ is a \textbf{global minimum} if $f(\vett x^*) \leq f(\vett x)$ for all $\vett x\in \R$, file the point is a \textbf{local minimum} if $f(\vett x^*) \leq f(\vett x)$ for $\vett x \in B(\vett x^*,\delta)$. In particular the minimum is \textbf{strict} defined then it means $f(\vett x^*) < f(\vett x)$.
	
	\paragraph{Necessary conditions} Necessary (but not sufficient) condition for a point $\vett x^* \in \R^n$ to be a local minimum is that
	\[ \nabla f(\vett x^*)^t = 0 \qquad \Rightarrow \]
	This relation does not give any information on the point if it's a minimum, a maximum or a saddle point and so a second order (or higher) derivative analyses is required.
	
	Assuming a function $f\in C^2(\R^n)$ (2 derivative continuos), if a point $\vett x^*\in R^n$ is a local minimum then $\nabla f(\vett x^*) = 0 $ and $\nabla^2 f(\vett x^*)$ is semi positive definite and so
	\[ \vett d^t\nabla^2f(\vett x^*) \vett d \geq 0 \qquad \forall d \in \R^n \]
	where $\nabla^2f(\vett x^*)$ is the hessian matrix of the function $f$. This condition (as the previous one) is necessary but not sufficient to determine if $\vstar x$ is a minimum or a saddle point. If the hessian is \textbf{positive defined}, so $ \nabla^2f(\vett x^*) > 0$, then the condition is also necessary and $\vstar x$ is a strict local minimum. In particular if the eigenvalues associated to $\nabla^2 f(\vstar x)$ are all positive, then the matrix is positive defined.
	
\section{Constrained minimization: Lagrange multipliers}
	The problem now is not to minimize a function $f\in C^2(\R^n)$ in all it's domain, but while considering a number $m$ of constraints defined by equations $h_k\in C^2(\R^n)$, so solving a problem in the form:
	\begin{align*}
		\textrm{minimize:} \qquad & f(\vett x) \\
		\textrm{with constraints}: \qquad & h_k(\vett x) = 0 \qquad k = 1,\dots, m
	\end{align*}
	
	\paragraph{Lagrange multiplier} The hard analytical problem of the constrained minimization can be solved using the \de{theorem of the Lagrange multiplier}. Let's consider a function $f$ to be minimized with a constraints map $\vett h$ (such that $f,\vett h \in C^2(\R^n)$) and let $\vstar x$ a local minimum of $f$ and satisfies all the constraints (and so $\vett h(\vstar x) = \vett 0$) then if $\nabla \vett h(\vstar x)$ has maximum rank, then there exists $m$ scalar $\lambda_k$ such that
	\begin{equation}
		\nabla f(\vstar x) - \sum_{k=1}^{m} \lambda_k \nabla h_k(\vstar x) = 0
	\end{equation}
	
	This problem reduces now to a form on where we need to compute the eigenvalues $\lambda_k$ of the \de{lagrangian} $\lag$ defined as 
	\begin{equation}
		\mathcal L (\vett x,\vett \lambda) := f(\vett x) - \sum_{k=1}^m \lambda_k h_k(\vett x) 
	\end{equation}
	In general the hardest part of the problem is determine all the points $\vstar x$ that satisfies the Lagrange multiplier conditions because that implies to solve a non linear system of equations that usually is very hard to explicitly express. However the second part of the problem is way much easier: we need in fact to compute the kernel (null space) of $\nabla \vett h(\vstar x)$ and, in order to have a local minimum, we have also to check that the matrix $\nabla_{\vett x} ^2\big(f(\vstar x) - \vett \lambda \, \vett h(\vstar x)\big)$ is semi positive defined. \vspace{3mm}
	
	Using the lagrangian definition, the constrained minimization problem can be reduced to the following system of equations:
	\begin{equation}
	\begin{cases}		
		\nabla_{\vett x} \lag(\vett x,\vett \lambda) = \nabla_{\vett x} f(\vett x) - \lambda^t\, \nabla_{\vett x} \vett h(\vett x) = \vett 0 \\
		\nabla_{\vett \lambda} \lag(\vett x, \vett \lambda) = \vett h(\vett x) = \vett 0
	\end{cases}
	\end{equation}
	All the points $\vstar x$ that satisfies this system are candidates to be local maximum/minimum (in fact by computing the gradient and setting it to zero we are indeed searching for the stationary points of the lagrangian).\\
	At this point to discriminate if the stationary point is maximum or minimum we have to use the second order conditions and in particular we must consider that the matrix $\nabla_{\vett x}^2 \lag(\vett x,\vett \lambda)$ is positive defined in the kernel of the constraints map $\vett h(\vett x)$, and so such that
	\begin{equation} \label{eq:min:secordnec}
		\vett z^t \, \nabla_{\vett x}^2\lag(\vett x,\vett \lambda)\, \vett z > 0 \qquad \forall \vett z \in \ker\{ \nabla\vett h(\vstar x) \}
	\end{equation}
	
	
	\paragraph{First and second order necessary condition} To summarise the first order necessary condition for the point to be a local minimum is that the gradient $\nabla f$ of the function to minimize should be inside the linear space generated by the gradients of the constraints:
	\[ \nabla f(\vstar x) \in \textrm{span} \big\{ \nabla h_1(\vstar x),\dots, \nabla h_m(\vstar x) \big\} \]
	
	The second order necessary condition is that the matrix $\nabla_{\vett x}^2 \lag(\vett x,\vett \lambda)$ is semi positive defined (and so it has to satisfy equation \ref{eq:min:secordnec}). In particular this condition is necessary when we consider an inequality of the type $\geq$, while the condition is sufficient when $\nabla_{\vett x}^2 \lag(\vett x,\vett \lambda) > 0$.
	
	\begin{example}{: constrained minimization problem}
		Let's consider the problem on where we want to minimize the function $f:\R^2\rightarrow R$ using the constraint $h$ defined as
		\[ f(x,y) = e^{x^2-y^2} \qquad,\qquad h(x,y) = x - y^2 \]
		
		In order to solve this problem we at first need to build the lagrangian (having only one constraint $\vett \lambda$ reduces to a scalar) and so
		\[ \lag(x,y,\lambda) = e^{x^2-y^2} - \lambda \big(x-y^2\big) \]
		We now need to compute the stationary points of the lagrangian and this means solving the following non linear system of equations:
		\[ \begin{cases}
			\nabla_x \lag(x,y,\lambda) = 2 x e^{x^2-y^2} - \lambda = 0 \\
			\nabla_y \lag(x,y,\lambda) = -2 y e^{x^2-y^2} + 2 \lambda y = 0 \\
			\nabla_\lambda \lag(x,y,\lambda) = -x + y^2 = 0 	
		\end{cases} \]
		\[ \Rightarrow \quad \big(x,y,\lambda\big) \quad  = \quad \left(0,0,0\right) , \left(\frac 1 2 , \frac 1 {\sqrt 2},e ^{-\frac 14}\right), \left(\frac 1 2 , -\frac 1 {\sqrt 2},e ^{-\frac 14}\right)\]
		
		To determine now it this points are local maximum or minimum we have to firstly define the general gradient of the constraint map and then the hessian of the Lagrangian in respect to the variable $\vett x =(x,y)$:
		\begin{align*}
			\nabla h(x,y) & = (1,-2y) \\
			\nabla^2_{(x,y)} \lag & = \begin{bmatrix}
				(4x^2+2)e^{x^2-y^2} & -4xy e^{x^2-y^2} \\
				-4xy e^{x^2-y^2} & (4y^2-2)e^{x^2-y^2} + 2\lambda
			\end{bmatrix}
		\end{align*}
		Now we have to check each stationary point independently:
		\begin{enumerate}
			\item when $x=y=\lambda = 0$ we have that $\nabla h = (1,0)$ while $\nabla^2_{(x,y)} \lag = \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix}$. By computing the null space of the vector $(1,0)$ we can see that all the vectors in the form $(0,\alpha)$ match the definition; we can now check if the point is of maximum/minimum be determining if the matrix $\nabla^2_{(x,y)} \lag$ is positive or negative defined:
			\[ \big(0 \ \ \alpha\big) \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix} \begin{pmatrix}
				0 \\ \alpha
			\end{pmatrix} = -2\alpha^2 \leq 0 \qquad \forall \alpha\in \R \]
			The hessian matrix is negative defined and so the point $(x,y) = (0,0)$ is a local maximum.
			
			\item evaluating for the second point $x = \frac 1 2$, $y = \frac{1}{\sqrt 2}$ and $\lambda = e^{-\frac 14}$ we can compute the gradient $\nabla h = (1, - \sqrt 2)$ of the constraint map that determines a null space of the form $(\alpha \sqrt 2,\alpha)$. Given the hessian matrix of the transform we can see that it's positive defined, in fact
			\[ e^{-\frac 1 4} \begin{pmatrix}
				\alpha \sqrt 2 & \alpha
			\end{pmatrix} \begin{bmatrix}
				3 & -\sqrt 2 \\ -\sqrt 2 & 2 
			\end{bmatrix} \begin{pmatrix}
				\alpha \sqrt 2 \\ \alpha
			\end{pmatrix} = 4 e^{-\frac 12} \alpha^2 > 0 \qquad \forall \alpha \in \R \]
			This means that the point is a local minimum.
			
			\item considering instead the last point $x = \frac 1 2$, $y = - \frac{1}{\sqrt 2}$ and $\lambda = e^{-\frac 14}$ we have a similar gradient $\delta h = (1,\sqrt 2)$ that determines a kernel in the form $(\alpha \sqrt 2,-\alpha)$. Evaluating the hessian on the null space base we can see that the matrix is positive defined, in fact
			\[ e^{-\frac 1 4} \begin{pmatrix}
				\alpha \sqrt 2 & -\alpha
			\end{pmatrix} \begin{bmatrix}
				3 & -\sqrt 2 \\ -\sqrt 2 & 2 
			\end{bmatrix} \begin{pmatrix}
				\alpha \sqrt 2 \\ -\alpha
			\end{pmatrix} = 4 e^{-\frac 12} \alpha^2 > 0 \qquad \forall \alpha \in \R \]
		\end{enumerate}
		
		We can see that the both points $\left(\frac 12, \frac 1{\sqrt 2}\right)$ and $\left(\frac 12, -\frac 1{\sqrt 2}\right)$ are local minimum and they are both also global minimum because we can see that $f\left(\frac 12, \frac 1{\sqrt 2}\right) = f\left(\frac 12, - \frac 1{\sqrt 2}\right) = e^{-\frac 1 4}$.
	\end{example}
	
	\subsection{Sylvester theorem}
		The tedious and error prone operation of finding the minimum with the lagrangian multiplier is the one that's performed to determine if the matrix is (or is not) semi positive defined in the kernel $\ker\{\nabla \vett h(\vstar x)\}$ of the gradient of the constraints map. In fact for every stationary point $\vstar x$ of the lagrangian we have to check that
		\[ \vett z^t \, \nabla_{\vett x}^2 \lag(\vstar x,\vstar\lambda) \, \vett z \geq 0 \qquad \forall \vett z\in \ker\{\nabla \vett h(\vstar x)\} \]
		
		For sake of simplicity from now we will denote the matrix $\nabla_{\vett x}^2\lag(\vstar x,\vstar{\lambda})$ as $A$. We can note that the vector $\vett z \in \ker\{\nabla \vett h(\vstar x)\}$ (and from now on we refer to the matrix $\nabla \vett h$ as $B$) can be expressed as a linear combination of the vectors $\vett k_i$ (that are representing the base of $B$) in the way
		\[ \vett z = \vett k_1 \alpha_1 + \vett k_2 \alpha_2 + \dots + \vett k_p \alpha_p = K \vett \alpha \qquad \vett \alpha \in \R^p \]
		We can see that this expression can be reduced to a multiplication of a matrix $K \in \R^{n\times p}$ (whose columns are the vector $\vett k_i$ of the kernel base) and a $p$-dimensional vector $\vett \alpha$ (where $p$ is the number of constraints in the map $\vett h(\vett x)$ ).
		
		We can now rewrite the second order necessary condition considering that
		\[ \vett z^t A \vett z = \vett \alpha^t K^t A K \vett \alpha = \vett \alpha^t C \vett \alpha  \qquad C := K^tAK \in \R^{p\times p} \]
		
		\begin{example}{}
			Let's consider the numerical problem when the values of the matrix $A = \nabla_{\vett x} \lag$ and $B=\nabla \vett h$ are given with values
			\[ A = \matrix{1 & 2 & 3 \\ 2 & 0 & 1 \\ 3 & 1 & 0} \qquad B = \matrix{1&0&0} \]
			
			The first thing now is to compute the manually compute the kernel of the matrix $B$ in $\R^3$ solving the linear system
			\[ \vector{1 &0 &0} \vector{z_1 \\ z_2 \\ z_3} = \vett 0 \qquad \Rightarrow \quad \begin{cases}
				z_1 = 0 \\ z_2 = \alpha \\ z_3 = \beta
			\end{cases} \qquad \alpha,\beta\in\R \]
			At this point we can rewrite the kernel of $B$ using the linear combination of the vector composing the base:
			\[ \ker\{B\} = \vector{0 \\ \alpha \\ \beta} = \vector{0 \\ 1 \\ 0} \alpha + \vector{0 \\ 0 \\ 1} \beta = \underbrace{\matrix{0 & 0 \\ 1 & 0 \\ 0 & 1}}_{=K} \vector{\alpha \\ \beta} \]
			The last thing is now to compute the matrix $C$ that should be analyzed to know if it's (semi) positive defined or not:
			\begin{align*}
				K^t AK& = \matrix{0 & 1 & 0 \\ 0 & 0 & 1} \matrix{1 & 2 & 3 \\ 2 & 0 & 1 \\ 3 & 1 & 0} \matrix{0 & 0 \\ 1 & 0 \\ 0 & 1} \\
				C & = \matrix{0&1\\1&0}
			\end{align*}
			
		\end{example}
	
		This definition reduces the complexity of the problem at analyzing the matrix $C$ (that's smaller than the original matrix $A$) and determining if that particular matrix is (semi) positive defined using two methods:
		\begin{enumerate}
			\item considering the fact that $C$ is a symmetric matrix we know for sure that it can be diagonalised with an expression $T^t \Lambda T$ (where $\Lambda$ is a diagonal matrix containing all the eigenvalues of $C$); considering the expression $\vett \alpha^t T^t \Lambda T \vett \alpha$ in order to have a semi positive defined matrix all the eigenvalue $\lambda_i$ must be positive or at least equals to zero. If it happens that $\lambda_i > 0 \ \forall i$, then the matrix is positive defined and the point is a local minimum;
			
			\item one other solution is to use the \de{Sylvester theorem} that states that\textit{ a symmetric matrix $A\in \R^{n\times n}$ is \textbf{positive defined} if and only if all the \textbf{principal minors} of $A$ are strictly \textbf{positive}} (note that if one minor is equal to zero, no information can be retrieved with this method).
		\end{enumerate} 
		
		\begin{example}{: application of the Sylvester theorem}
			Let's consider now the matrix
			\[ A = \matrix{1 & 2 \\ 2 & 5} \]
			In order to determine if $A$ is positive defined we have to compute all the principle minors starting from the first $A_1$ that's represented only by the element $a_{11}$ of $A$ and so
			\[ A_1 = \det \matrix{1} = 1 > 0 \]
			The second (and last) principle minor of $A$ is the determinant of the whole matrix and it happens that
			\[ A_2 = \det \matrix{1 & 2 \\ 2 & 5} = 1 > 0\]
			Having all minors greater then zero, this means that the matrix $A$ is positive defined. \vspace{3mm}
			
			The same result can be achieved by determining all the eigenvalues of the matrix and showing they are all positive; we can in fact see that the characteristic polynomial of $A$ is equal to
			\[ p(\lambda) = \det\big[A-\lambda I\big] = \det \matrix{1-\lambda & 2 \\ 2 & 5-\lambda} = \lambda^2 - 6\lambda + 1 \]		
			\[ \lambda_{1,2} = \frac{6\pm \sqrt{36-4}}{2} = 3 \pm \frac{\sqrt{32}}{2} \]
			nothing that $\frac{\sqrt{32}}{2} < \frac{\sqrt{36}}{2}	= 3$, then all the eigenvalues $\lambda_1,\lambda_2$ are strictly positive and so $A$ is positive defined.
		\end{example}
		
		\paragraph{Trick for semi positive matrices} The Sylvester theorem gives no hint to determine if a matrix is semi-positive defined when a minor is equal to zero. However a way to determine if the matrix is semi-positive defined is by considering that the matrix $A +\varepsilon I$ should be positive defined for values of $\varepsilon$ approaching zero from the positive direction (for $\varepsilon\rightarrow 0^+$).
		
		\begin{example}{: usage of the Sylvester theorem trick}
			Let's consider now the matrix
			\[ A = \matrix{1 & 2 \\ 2 & 4} \]
			The first minor $A_1 = \det[1] = 1$ is positive, and so we have to compute the second principal minor noting that it's equal to zero:
			\[ A_2 = \det\matrix{1 & 2 \\ 2 & 4} = 4 - 2\cdot 2 = 0 \]
			This relation gives no clue on determining if $A$ is positive defined or not, but considering the trick yet defined we can compute 
			\[ \det\matrix{1+\varepsilon & 2 \\ 2 & 4 + \varepsilon} = (1+ \varepsilon)(4+\varepsilon) - 4 = 5\varepsilon \]
			We can see that this expression, for $\varepsilon \rightarrow 0^+$, is strictly greater then zero and this might conclude our analyses stating that $A$ is semi positive defined. \vspace{3mm}
			
			The same result can be defined by computing the eigenvalues of $A$ and so calculating the roots of the polynomial
			\[ p(\lambda) = \matrix{1-\lambda & 2 \\ 2 & 4-\lambda} = (1-\lambda)(4-\lambda) - 4 = \lambda^2-5\lambda \qquad \Rightarrow \quad \lambda_1 = 0, \lambda_2 = 5\]
			Having one eigenvalues zero (and the other positive) determines that $A$ is semi positive defined.			
		\end{example}
		\begin{example}{: counter example of the Sylvester theorem}
			Let's consider now the matrix
			\[ A = \matrix{ 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 0}  \]
			The first minor $A_1 = \det[1] = 1$ is positive, while the second one is zero, in fact
			\[ A_2 = \det\matrix{1 & 1 \\ 1 & 1} = 0 \]
			This result gives no information but we can use the trick by considering the determinant
			\[ \det\matrix{1 + \varepsilon & 1 \\ 1 & 1 + \varepsilon} = (1+\varepsilon)^2 - 1 = \varepsilon^2 +2\varepsilon > 0 \qquad \textrm{for } \varepsilon > 0  \]
			Being $A$ a $3\times3$ matrix we also need to compute the third minor that leads to another zero:
			\[ A_3 = \det\matrix{ 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 0} = 1 \det\matrix{1&1\\1&1} - 1 \det\matrix{1&1\\1&1} + 0 \det\matrix{1&1\\1&1} = 0  \]
			We can use another time the Sylvester theorem trick considering the determinant
			\[ p(\varepsilon) = \det\matrix{ 1+\varepsilon & 1 & 1 \\ 1 & 1+\varepsilon & 1 \\ 1 & 1 & \varepsilon} = (1+\varepsilon)\big(\varepsilon^2 + \varepsilon - 1\big) - (\varepsilon-1) - \varepsilon = \varepsilon\big(\varepsilon^2 + 2\varepsilon - 2\big) \]
			Having that the derivative $p'(\varepsilon) =3\varepsilon^2 + 4\varepsilon -2$ is negative for $\varepsilon$ approaching zero, then we can conclude that $A$ is not semi positive defined. \vspace{3mm}
			
			The same result can be confirmed calculating the eigenvalues of the matrix and so
			\[ p(\lambda) = \det\matrix{ 1 -\lambda & 1 & 1 \\ 1 & 1 -\lambda & 1 \\ 1 & 1 & -\lambda} = -\lambda\big(\lambda^2 - 2\lambda - 2\big)  \]
			\[ \Rightarrow \qquad \lambda_1 = 0 \qquad \lambda_{2,3} = \frac{2\pm \sqrt{12}}{2} = 1 \pm \sqrt 3 \]
			Noting that $\lambda_3 = 1-\sqrt 3 <0$ is negative, than the matrix $A$ is for sure not semi positive defined.
		\end{example}
	
	
\section{Inequality constraints}
	Let's introduce now the problem of minimizing a function $f(\vett x):\R^n\rightarrow R$ considering a set of $p$ inequalities constrains in the form $g_k(\vett x) \geq 0$.
	\begin{align*}
		\textrm{minimize:} \qquad & f(\vett x) \\
		\textrm{subject to:}\qquad & g_k(\vett x) \geq 0 \qquad k = 1,\dots,p
	\end{align*}
	The first approach to this problem is by trying to transform the inequality constraints $g_k$ into equality one (so having $h_k(\vett x) = 0$). This can be done considering that each constraint can be expressed as
	\[ g_k(\vett x) = \varepsilon_k^2 \geq 0 \qquad \Rightarrow h_k(\vett x,\varepsilon_k) = g_k(\vett x) - \varepsilon_k^2 = 0  \]
	Doing this process for each constraint we can determine a minimization problem that depends both on the function variable $\vett x$ but also on the so called \textbf{slack variables} $\vett \varepsilon$:
	\begin{align*}
		\textrm{minimize:} \qquad & f(\vett x) \\
		\textrm{subject to:}\qquad & h_k(\vett x, \vett \varepsilon) = 0 \qquad k = 1,\dots,p
	\end{align*}
	This kind of problem increases the computational costs because it increases the variable to minimize from $n$ to $n+p$.
	\begin{note}
		We used the expression $\varepsilon_k^2$ and not $\varepsilon_k$ because with this expression we don't need to specify one more inequality $\varepsilon_k \geq 0$.
	\end{note}
	\begin{example}{} \label{ex:min:inequ}
		Let's consider the following problem:\begin{align*}
			\textrm{minimize:} \qquad & f(x,y) = x^2 \\
			\textrm{subject to:}\qquad & x^2+y^2\leq 1 \\ & x+y \geq 0
		\end{align*}
		In order to solve this problem we have to reduce the inequality constraints to equality ones; considering the first constraint, that has to be expressed in the form $g_1 \geq 0$ and so
		\[ g_1(x,y) = 1 -x^2-y^2 \geq 0 \qquad \Rightarrow \quad h_1(x,y) = 1-x^2-y^2-\varepsilon_1^2 \]
		At the same way it's possible to express the second inequality as the a constraint $h_2(x,y,\varepsilon_2) = x+y-\varepsilon_2^2$. With this expression being set the problem reduces to the form\begin{align*}
			\textrm{minimize:} \qquad & f(x,y,\epsilon_1,\epsilon_2) = x^2 \\
			\textrm{subject to:}\qquad & h_1(x,y,\epsilon_1,\epsilon_2) = 1- x^2 - y^2  - \epsilon_1^2 = 0 \\ & h_2(x,y,\epsilon_1,\epsilon_2) = x+y - \varepsilon_2^2 = 0
		\end{align*}
		With the problem this stated we can build the lagrangian that depend's on both the original variables $x,y$ but also the slack variables $\epsilon_1,\eps 2$ and so
		\begin{align*}
			\lag(\underbrace{x,y,\eps1,\eps2}_{\vett\epsilon},\underbrace{\lambda_1,\lambda_2}_{\vett\lambda}) &= f(\vett x) - \lambda_1 h_1(\vett x) - \lambda_2 h_2(\vett x) \\
			& = x^2 - \lambda_1 \big( 1- x^2-y^2-\eps1^2 \big) - \lambda_2 \big( x+y-\eps2^2 \big)
		\end{align*}
		To find the local minimum point we can start using the first necessary condition of the lagrangian multiplier, so by determining all the stationary point such that $\nabla \lag = \vett 0$; in practise this means solving the following non linear system of 6 equations in 6 variables:
		\[ \begin{cases}
			\pd \lag x =& 2x -2\lambda_1x -\lambda_2  = 0 \\
			\pd \lag y =& 2\lambda_1 y - \lambda_2  = 0\\
			\pd \lag {\eps1} =& 2\lambda_1\eps 1  = 0 \\
			\pd \lag {\eps2} =& 2\lambda_2 \eps 2  =  0 \\
			\pd \lag {\lambda_1} =& x^2+y^2 + \eps 1 ^2-1  = 0 \\
			\pd \lag {\lambda_2} =& x + y - \eps 2^2 = 0
		\end{cases} \]
		Solving minimization problems with inequality constraints can be tricky using the lagrangian multiplier method and so other algorithm (like the \kkt that's going to be explained) are usually preferred. However this kind of system can be solved by cases; considering in fact the second and third equations we can discriminate the solution considering the various combination when $\eps1,\eps 2$ are equal (or not) to zero.
		
		Considering for example the case $\eps 1 = \eps 2 = 0$ the non linear system reduces to the form
		\[\begin{cases}
			2x - 2\lambda_1 x - \lambda_2 = 0 \\ 
			2\lambda_1 y - \lambda_2 = 0 \\
			x^2+y^2-1 = 0 \\ 
			x + y = 0 
		\end{cases}\]
		Considering that the first two expressions are linear in respect to $\lambda_1,\lambda_2$ it's possible to determine their value directly in terms of $x,y$:
		\[ \lambda_1 = \frac{x}{y-x} \qquad \lambda_2 = 2 \frac{xy}{y-x} \]
		Considering instead the last 2 equations we have a non linear system that can be also computed by cases; from the last equation we have in fact that $y = -x$ and so
		\[ 1-x^2-x^2 = 0 \qquad \Rightarrow \quad x = \pm \frac{1}{\sqrt 2} \]
		At this end of this process we defined 2 stationary points for the lagrangian:
		\[ \big(x,y,\eps1,\eps2,\lambda_1,\lambda_2\big) \quad = \quad \left( \frac 1 {\sqrt 2} , - \frac{1}{\sqrt 2},0,0,-\frac 1 2, \frac{\sqrt 2}{2} \right) , \left( - \frac 1 {\sqrt 2} , \frac{1}{\sqrt 2},0,0,\frac 1 2, -\frac{\sqrt 2}{2} \right) \]
		
		Solving symbolically this system of equation with a computer the lonely real solution of the problem (that determines a local minimum) is expressed in the form
		\[ \big(x,y,\eps1,\eps2,\lambda_1,\lambda_2\big) \quad = \quad \left( 0, \sqrt{-\eps1^2 + 1 }, \eps 1 ,\sqrt[4]{1 - \eps1^2}, 1, 1 \right) \]
		In order for the square roots to exists the value $\eps 1^2$ should fit inside the range $[0,1]$ and so the local minimum of the problem can be found on the vertical line
		\[ (x,y) = (0,k) \qquad k \in [0,1] \]
		
		
	\end{example}
	
	
	
\section{Karush-Kuhn-Tucker conditions}
	The \de{\kkt} (KKT) \de{conditions} are a set of necessary and sufficient conditions (first and second order) that allows to describe if a point $\vstar x$ is a minimum point of a minimization problem subjected to equality and inequality constraints, so for the problem
	\begin{align*}
		\textrm{minimize:} \qquad & f(\vett x) \\
		\textrm{subject to:}\qquad  \ & h_k(\vett x) = 0 \qquad && k=1,\dots, m\\
		& g_k(\vett x) \geq 0  &&k = 1,\dots,p
	\end{align*}
	where $f,h_k,g_k: \R^n\rightarrow \R$ are real evaluated function.
	
	In order to describe this conditions it's important to define \textbf{set of the active constraints} $\act$ as
	\[ \act(\vstar x) = \big\{ k \ | \ g_k(\vstar x) = 0 \big\} \]
	This means that given the constraints $g\in C(\R^n)$ it's defined as \textit{active} if it happens that the point $\vstar x$ is such that $g(\vstar x) = 0$, so in practical way it's in the \textit{border} between activating and not activating the constraints.
	
	\paragraph{First order necessary condition} Lef $f\in C^1(\R^n)$ a function to minimize subjected to the inequality $\vett g\in C^1(\R^n,\R^p)$ and equality $\vett h \in C^1(\R^n,\R^m)$ constraint maps. If $\vstar x$ satisfy constraint qualification then necessary condition for $\vstar x$ to be a local minima is that that exists $m+p$ scalars $\lambda_i,\mu_j$ such that all the following conditions are satisfied:
	\begin{equation} \label{eq:min:kkt:firstnecessary}
	\begin{aligned}
		\nabla_{\vett x} \lag \big(\vstar x,\vstar \lambda,\vstar \mu\big) & = \vett 0 \\
		h_k\big(\vstar x\big) & =  0 \qquad && k = 0,\dots, m\\
		\mu_k^* g_k\big(\vstar x\big) & = 0 && k = 0,\dots, p \\
		\mu_k^* & \geq 0 && k = 0,\dots,p
	\end{aligned}
	\end{equation}
	where the lagrangian is in this case defined as
	\[ \lag(\vett x,\vett \lambda,\vett \mu) = f(\vett x) - \sum_{k=1}^p \mu_k g_k(\vett x)  - \sum_{k=1}^m \lambda_k h_k(\vett x) \]
	
	\begin{example}{}
		Solving minimization problem with the KKT conditions is generally more easier; considering the case of the minimization problem in example \ref{ex:min:inequ} the lagrangian will depend only on 4 parameter (instead of 6 as in that example):
		\[ \lag(x,y,\mu_1,\mu_2) = x^2 - \mu_1\big(1-x^2-y^2\big) - \mu_2\big(x+y\big) \]
		Using the first order KKT condition the main non linear system to solve is in the form
		\[\left\{\begin{aligned}
			\left. \begin{aligned}				
				2x +2\mu_1x - \mu_2 &= 0 \\
				2 \mu_1 y - \mu_2 &= 0
			\end{aligned} \right\} & \quad \nabla_{\vett x} \lag\big(\vett x, \vett \lambda\big)  	\\		
			\left. \begin{aligned}
				\mu_1\big(1-x^2-y^2\big) & = 0 \\
				\mu_2 \big(x+y\big) & = 0
			\end{aligned} \right\} &  \quad \mu_k g_k\big(\vett x \big) = 0\\
			\mu_1,\mu_2 \geq 0 \quad & 
		\end{aligned} \right.\]
		This system of four non linear equations (the conditions $\mu_1,\mu_2\geq0$ are \textit{easier} to consider) can be solved by cases:
		\begin{itemize}
			\item if $\mu_1 = \mu_2 = 0$ then the last two equalities are always verified, while the first two becomes
			\[\begin{cases}
				2x = 0 \\
				0 = 0 
			\end{cases}\]
			this means that $x = 0$, while $y$ is a free parameter that must satisfy both the constraints $x+y\geq 0 $ (so that $y \geq 0$) and $1-x^2-y^2 \geq 0 $ (and so $1-y^2 \geq 0 $ that determines $-1 \leq y \leq 1$): considering both condition at the same time we have that
			\[ \begin{cases}
				y \geq 0 \\ -1 \leq y \leq 1 
			\end{cases} \qquad \Rightarrow \quad \textrm{solution: } x = \mu_1 = \mu_2 = 0 \quad y \in [0,1]\]
			
			\item when considering $\mu_1 \neq 0$ and $\mu_2 = 0$ the system of equations become
			\[\begin{cases}
				2x + 2 \mu_1x = 0 \\
				2\mu_1 y = 0 \\
				\mu_1\big(1-x^2-y^2\big) = 0
			\end{cases}\]
			Based on the assumption of the value $\mu_1$, from the second equation we determine that $y$ must be equal to $0$ and so the third equation becomes $1-x^2 = 0$ whose solutions are $\pm 1$. Considering that the first equation can be factorised as $2x(1+\mu_1) = 0$ (and $x\neq 0$), then in order to satisfy the relation $\mu_1$ must be $-1$, but this imply that $\mu_1$ is negative and it's an unacceptable solution, so for $\mu_1 \neq 0,\mu_2=0$ no solutions of the system can be found;
			
			\item considering instead $\mu_1 = 0$ and $\mu_2 \neq 0$ the system becomes
			\[ \begin{cases}
				2x - \mu_2 = 0 \\ 
				-\mu_2 = 0 \\
				\mu_2(x+y) = 0
			\end{cases} \]
			The second equation states that $\mu_2$ must be equal to $0$ and so the solution of this problem is the same as the first case considered;
			
			\item considering both variables $\mu_1,\mu_2 \neq 0$ we have to solve the full systems; considering the last two equations we have that
			\[ \begin{cases}
				x+y = 0 \\ 1-x^2-y^2 = 0
			\end{cases} \qquad \Rightarrow \quad x = -y \quad \Rightarrow \quad 1-2x^2 = 0\]
			and so it means that the two possible solution for the systems are $(x,y) = \left( \pm \frac{1}{\sqrt{2}}, \mp \frac{1}{\sqrt{2}} \right)$. Considering the first case where $x = 1/\sqrt 2$ the first two equations become
			\[ \begin{cases}
				\frac{2}{\sqrt 2} (1+\mu_1) - \mu_2 = 0 \\
				-\frac 2 {\sqrt 2} \mu_1 - \mu_2 = 0
			\end{cases} \qquad \Rightarrow \quad \mu_2 = - \frac 2 {\sqrt 2} \mu_1 \]
			\[ \Rightarrow \quad \frac{4}{\sqrt 2} \mu_1 + \frac{2}{\sqrt 2} = 0 \qquad \Rightarrow \quad \mu_1 = - \frac 1 2 \qquad \mu_2 = \frac 1 {\sqrt 2}   \]
			In this case $\mu_1 < 0 $ is an unacceptable solution of the problem, and also considering the point $x = -1/\sqrt 2$ the system becomes with no considerable solutions
			\[ \begin{cases}
				-\frac{2}{\sqrt 2} (1+\mu_1) - \mu_2 = 0 \\
				\frac 2 {\sqrt 2} \mu_1 - \mu_2 = 0
			\end{cases} \qquad \Rightarrow \quad \mu_1 = -\frac 12\quad \mu_2 =  - \frac 1 {\sqrt 2}  \]			
		\end{itemize}
		With all this considerations done the point of local minimum relies on the points
		\[ (x,y) = (0,k) \qquad k\in[0,1] \]
	\end{example}
	
	\paragraph{Second order necessary conditions} Given a function $f \in C^1(\R^n)$ to minimize subject to the equality $\vett h\in C^1(\R^n,\R^m)$ and inequality $\vett g \in C^1(\R^n,\R^p)$ constraint maps, then necessary conditions for a point $\vstar x$ (that satisfies the constraints) to be a local minima is that exists $m+p$ scalars $\lambda_i,\mu_i$ such that satisfy the first order conditions (equation \ref{eq:min:kkt:firstnecessary}) and
	\begin{equation}
		\vett z^t \, \nabla^2_{\vett x} \lag\big(\vstar x,\vstar \lambda,\vstar \mu\big) \, \vett z \geq 0
	\end{equation}	
	for all vector $\vett z$ such that
	\begin{align*}
		i) \qquad && \nabla h_k(\vstar x) \vett z & = 0 && k = 1,\dots,m \\
		ii) \qquad &&\nabla g_k(\vstar x) \vett z & = 0 && \textrm{for all } k\in \act(\vstar x) \textrm{ and } \mu_k > 0 \\
		iii) \qquad &&\nabla g_k(\vstar x) \vett z & \geq 0 && \textrm{for all } k\in \act(\vstar x) \textrm{ and } \mu_k = 0
	\end{align*}
	In general the conditions $ii)$ and $iii)$ are hard to verify and so to have a less accurate necessary condition (by having a smaller set of possible vector $\vett z$), and so improving the chance of considering point that don't belong to the original domain, this two conditions can be substitute with the expression
	\[ \nabla g_k(\vstar x) \vett z = 0 \qquad \textrm{for all } k \in \act(\vstar x) \]
	
	\paragraph{Second order sufficient conditions} Given a function $f \in C^1(\R^n)$ to minimize subject to the equality $\vett h\in C^1(\R^n,\R^m)$ and inequality $\vett g \in C^1(\R^n,\R^p)$ constraint maps, then sufficient condition for a point $\vstar x$ (that satisfies the constraints) to be a local minima is that exists $m+p$ scalars $\lambda_i,\mu_i$ such that satisfy the first order conditions (equation \ref{eq:min:kkt:firstnecessary}) and 
	\begin{equation}
		\vett z^t \, \nabla^2_{\vett x} \lag\big(\vstar x,\vstar \lambda,\vstar \mu\big) \, \vett z > 0
	\end{equation}	
	for all vector $\vett z$ such that
	\begin{align*}
		i) \qquad && \nabla h_k(\vstar x) \vett z & = 0 && k = 1,\dots,m \\
		ii) \qquad &&\nabla g_k(\vstar x) \vett z & = 0 && \textrm{for all } k\in \act(\vstar x) \textrm{ and } \mu_k > 0 \\
		iii) \qquad &&\nabla g_k(\vstar x) \vett z & \geq 0 && \textrm{for all } k\in \act(\vstar x) \textrm{ and } \mu_k = 0
	\end{align*}
	As in the previous case in order to have simpler calculation the third equation $iii)$ can be dropped (and so we obtain less accurate solution of the local minima).
	
	\begin{example}{}
		\begin{align*}
			\textrm{minimize:}&&   f(x,y) &= x^2 - xy \\
			\textrm{subject to:}&& g_1(x,y) &=  1-x^2-y^2 \geq 0 \\ 
					&& g_2(x,y) &=  1- (x-1)^2-y^2 \geq 0
		\end{align*}
		To solve this kind of problem using the KKT conditions we firstly need to build the lagrangian of the problem that's equal to
		\[ \lag(x,y,\mu_1,\mu_2) = x^2-xy - \mu_1 \big(1-x^2-y^2\big) - \mu_2 \big[ 1- (x-1)^2 - y^2 \big] \]
		At this point it's possible to construct the non linear system of equation whose solution are the stationary points candidate to be local minimum:
		\[\left\{\begin{aligned}
			\left. \begin{aligned}				
				2x - y +2\mu_1 x + 2\mu_2(x-1) &= 0 \\
				-x + 2 \mu_1 y +2 \mu_2 y &= 0
			\end{aligned} \right\} & \quad \nabla_{\vett x} \lag\big(\vett x, \vett \lambda\big)  	\\		
			\left. \begin{aligned}
				\mu_1 \big(1-x^2-y^2\big) & = 0 \\
				\mu_2 \big[ 1- (x-1)^2 - y^2 \big] & = 0
			\end{aligned} \right\} &  \quad \mu_k g_k\big(\vett x \big) = 0\\
			\mu_1,\mu_2 \geq 0 \quad & 
		\end{aligned} \right.\]
		This system can be solved by cases of the value of $\mu_i$ and in particular
		\begin{itemize}
			\item when $\mu_1 = \mu_2 = 0$ the system reduces to the form
			\[ \begin{cases}
				2x - y = 0 \\ - x = 0
			\end{cases}  \qquad \Rightarrow \quad x = y = 0 \]
			This solution satisfy the constraints, in fact $g_1(0,0) = 1 \geq 0$ and $g_2(0,0) = 0 \geq 0$, so this point can be used to compute the second order conditions;
			
			\item considering all the other cases of $\mu_j$ the only other analytical solution to the system, found with \texttt{Mathematica}, determines the point
			\[ (x,y,\mu_1,\mu_2) = \left( \frac 1 2, \frac{\sqrt 3}{2}, \frac 1{\sqrt 3} - \frac 12 , \frac 1 2 - \frac{\sqrt 3}{6} \right) \]			
		\end{itemize}
		In order to consider now the second order necessary/sufficient conditions we have firstly to evaluate the gradient $\nabla \vett g$ of the inequality constraint map (in respect to the variables $x,y$) resulting in the matrix
		\[ \nabla \vett g = \matrix{ -2x & -2y \\ -	2(x-1) & -2y^2 }\]
		For the second order conditions is also important to define the hessian matrix of the lagrangian respect to the variables $x,y$:
		\[ \nabla_{\vett x}^2 \lag = \matrix{ 2 + 2\mu1 + 2 \mu2 & -1 \\ -1 & 2\mu_1 + 2\mu_2 }  \]
		
		We have now to check the conditions for the two local minima candidates:
		\begin{itemize}
			\item in the first case when $x=y = \mu_1 = \mu_2 = 0$ the set of the active constraints is just the second inequality $g_2$: in fact we have that $g_1(0,0) = 1 \neq 0$ while $g_2(0,0) = 0$. In respect to this vector we have to compute it's kernel (to determine the direction $z$ on which verify the KKT condition) and in particular
			\[ \nabla g_2(0,0) = \vector{0\\ 2} \qquad \Rightarrow \quad \ker\big\{ \nabla g_2(0,0) \} = \alpha \vector{1 \\ 0} \]
			Evaluating the expression $\vett z^t \, \nabla_{\vett x}^2\lag \, \vett z$ for $z\in \ker{\nabla g_2}$ we obtain that
			\[ \alpha \vector{1 & 0} \matrix{ 2 & - 1 \\ - 1 & 2 } \vector{1 \\ 0} \alpha = 2\alpha^2  \]
		\end{itemize}
		
	\end{example}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	