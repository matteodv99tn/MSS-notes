\chapter{Constrained Minimization}

\section*{Calculus revision}
	Given a function $f:\mathds R^n\rightarrow \mathds R$ it's possible to compute it's \textbf{minimum} by doing the assumption that $f$ has a \textbf{Lipschitz continuos gradient}, notated as $f\in C^1(\R^n)$, meaning that
	\[ \exists \, \gamma > 0 \quad \textrm{such that} \quad \left\| \nabla f(\vett x)^t - \nabla f(\vett y)^t\right\| \leq \gamma \| \vett x - \vett y\| \qquad \forall \vett x,\vett y \in \R^n  \]
	
	A point $\vett x^*\in R$ is a \textbf{global minimum} if $f(\vett x^*) \leq f(\vett x)$ for all $\vett x\in \R$, file the point is a \textbf{local minimum} if $f(\vett x^*) \leq f(\vett x)$ for $\vett x \in B(\vett x^*,\delta)$. In particular the minimum is \textbf{strict} defined then it means $f(\vett x^*) < f(\vett x)$.
	
	\paragraph{Necessary conditions} Necessary (but not sufficient) condition for a point $\vett x^* \in \R^n$ to be a local minimum is that
	\[ \nabla f(\vett x^*)^t = 0 \qquad \Rightarrow \]
	This relation does not give any information on the point if it's a minimum, a maximum or a saddle point and so a second order (or higher) derivative analyses is required.
	
	Assuming a function $f\in C^2(\R^n)$ (2 derivative continuos), if a point $\vett x^*\in R^n$ is a local minimum then $\nabla f(\vett x^*) = 0 $ and $\nabla^2 f(\vett x^*)$ is semi positive definite and so
	\[ \vett d^t\nabla^2f(\vett x^*) \vett d \geq 0 \qquad \forall d \in \R^n \]
	where $\nabla^2f(\vett x^*)$ is the hessian matrix of the function $f$. This condition (as the previous one) is necessary but not sufficient to determine if $\vstar x$ is a minimum or a saddle point. If the hessian is \textbf{positive defined}, so $ \nabla^2f(\vett x^*) > 0$, then the condition is also necessary and $\vstar x$ is a strict local minimum. In particular if the eigenvalues associated to $\nabla^2 f(\vstar x)$ are all positive, then the matrix is positive defined.
	
\section{Constrained minimization: Lagrange multipliers}
	The problem now is not to minimize a function $f\in C^2(\R^n)$ in all it's domain, but while considering a number $m$ of constraints defined by equations $h_k\in C^2(\R^n)$, so solving a problem in the form:
	\begin{align*}
		\textrm{minimize:} \qquad & f(\vett x) \\
		\textrm{with constraints}: \qquad & h_k(\vett x) = 0 \qquad k = 1,\dots, m
	\end{align*}
	
	\paragraph{Lagrange multiplier} The hard analytical problem of the constrained minimization can be solved using the \de{theorem of the Lagrange multiplier}. Let's consider a function $f$ to be minimized with a constraints map $\vett h$ (such that $f,\vett h \in C^2(\R^n)$) and let $\vstar x$ a local minimum of $f$ and satisfies all the constraints (and so $\vett h(\vstar x) = \vett 0$) then if $\nabla \vett h(\vstar x)$ has maximum rank, then there exists $m$ scalar $\lambda_k$ such that
	\begin{equation}
		\nabla f(\vstar x) - \sum_{k=1}^{m} \lambda_k \nabla h_k(\vstar x) = 0
	\end{equation}
	
	This problem reduces now to a form on where we need to compute the eigenvalues $\lambda_k$ of the \de{lagrangian} $\lag$ defined as 
	\begin{equation}
		\mathcal L (\vett x,\vett \lambda) := f(\vett x) - \sum_{k=1}^m \lambda_k h_k(\vett x) 
	\end{equation}
	In general the hardest part of the problem is determine all the points $\vstar x$ that satisfies the Lagrange multiplier conditions because that implies to solve a non linear system of equations that usually is very hard to explicitly express. However the second part of the problem is way much easier: we need in fact to compute the kernel (null space) of $\nabla \vett h(\vstar x)$ and, in order to have a local minimum, we have also to check that the matrix $\nabla_{\vett x} ^2\big(f(\vstar x) - \vett \lambda \, \vett h(\vstar x)\big)$ is semi positive defined. \vspace{3mm}
	
	Using the lagrangian definition, the constrained minimization problem can be reduced to the following system of equations:
	\begin{equation}
	\begin{cases}		
		\nabla_{\vett x} \lag(\vett x,\vett \lambda) = \nabla_{\vett x} f(\vett x) - \lambda^t\, \nabla_{\vett x} \vett h(\vett x) = \vett 0 \\
		\nabla_{\vett \lambda} \lag(\vett x, \vett \lambda) = \vett h(\vett x) = \vett 0
	\end{cases}
	\end{equation}
	All the points $\vstar x$ that satisfies this system are candidates to be local maximum/minimum (in fact by computing the gradient and setting it to zero we are indeed searching for the stationary points of the lagrangian).\\
	At this point to discriminate if the stationary point is maximum or minimum we have to use the second order conditions and in particular we must consider that the matrix $\nabla_{\vett x}^2 \lag(\vett x,\vett \lambda)$ is positive defined in the kernel of the constraints map $\vett h(\vett x)$, and so such that
	\begin{equation} \label{eq:min:secordnec}
		\vett z^t \, \nabla_{\vett x}^2\lag(\vett x,\vett \lambda)\, \vett z > 0 \qquad \forall \vett z \in \ker\{ \nabla\vett h(\vstar x) \}
	\end{equation}
	
	
	\paragraph{First and second order necessary condition} To summarise the first order necessary condition for the point to be a local minimum is that the gradient $\nabla f$ of the function to minimize should be inside the linear space generated by the gradients of the constraints:
	\[ \nabla f(\vstar x) \in \textrm{span} \big\{ \nabla h_1(\vstar x),\dots, \nabla h_m(\vstar x) \big\} \]
	
	The second order necessary condition is that the matrix $\nabla_{\vett x}^2 \lag(\vett x,\vett \lambda)$ is semi positive defined (and so it has to satisfy equation \ref{eq:min:secordnec}). In particular this condition is necessary when we consider an inequality of the type $\geq$, while the condition is sufficient when $\nabla_{\vett x}^2 \lag(\vett x,\vett \lambda) > 0$.
	
	\begin{example}{: constrained minimization problem}
		Let's consider the problem on where we want to minimize the function $f:\R^2\rightarrow R$ using the constraint $h$ defined as
		\[ f(x,y) = e^{x^2-y^2} \qquad,\qquad h(x,y) = x - y^2 \]
		
		In order to solve this problem we at first need to build the lagrangian (having only one constraint $\vett \lambda$ reduces to a scalar) and so
		\[ \lag(x,y,\lambda) = e^{x^2-y^2} - \lambda \big(x-y^2\big) \]
		We now need to compute the stationary points of the lagrangian and this means solving the following non linear system of equations:
		\[ \begin{cases}
			\nabla_x \lag(x,y,\lambda) = 2 x e^{x^2-y^2} - \lambda = 0 \\
			\nabla_y \lag(x,y,\lambda) = -2 y e^{x^2-y^2} + 2 \lambda y = 0 \\
			\nabla_\lambda \lag(x,y,\lambda) = -x + y^2 = 0 	
		\end{cases} \]
		\[ \Rightarrow \quad \big(x,y,\lambda\big) \quad  = \quad \left(0,0,0\right) , \left(\frac 1 2 , \frac 1 {\sqrt 2},e ^{-\frac 14}\right), \left(\frac 1 2 , -\frac 1 {\sqrt 2},e ^{-\frac 14}\right)\]
		
		To determine now it this points are local maximum or minimum we have to firstly define the general gradient of the constraint map and then the hessian of the Lagrangian in respect to the variable $\vett x =(x,y)$:
		\begin{align*}
			\nabla h(x,y) & = (1,-2y) \\
			\nabla^2_{(x,y)} \lag & = \begin{bmatrix}
				(4x^2+2)e^{x^2-y^2} & -4xy e^{x^2-y^2} \\
				-4xy e^{x^2-y^2} & (4y^2-2)e^{x^2-y^2} + 2\lambda
			\end{bmatrix}
		\end{align*}
		Now we have to check each stationary point independently:
		\begin{enumerate}
			\item when $x=y=\lambda = 0$ we have that $\nabla h = (1,0)$ while $\nabla^2_{(x,y)} \lag = \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix}$. By computing the null space of the vector $(1,0)$ we can see that all the vectors in the form $(0,\alpha)$ match the definition; we can now check if the point is of maximum/minimum be determining if the matrix $\nabla^2_{(x,y)} \lag$ is positive or negative defined:
			\[ \big(0 \ \ \alpha\big) \begin{bmatrix} 2 & 0 \\ 0 & -2 \end{bmatrix} \begin{pmatrix}
				0 \\ \alpha
			\end{pmatrix} = -2\alpha^2 \leq 0 \qquad \forall \alpha\in \R \]
			The hessian matrix is negative defined and so the point $(x,y) = (0,0)$ is a local maximum.
			
			\item evaluating for the second point $x = \frac 1 2$, $y = \frac{1}{\sqrt 2}$ and $\lambda = e^{-\frac 14}$ we can compute the gradient $\nabla h = (1, - \sqrt 2)$ of the constraint map that determines a null space of the form $(\alpha \sqrt 2,\alpha)$. Given the hessian matrix of the transform we can see that it's positive defined, in fact
			\[ e^{-\frac 1 4} \begin{pmatrix}
				\alpha \sqrt 2 & \alpha
			\end{pmatrix} \begin{bmatrix}
				3 & -\sqrt 2 \\ -\sqrt 2 & 2 
			\end{bmatrix} \begin{pmatrix}
				\alpha \sqrt 2 \\ \alpha
			\end{pmatrix} = 4 e^{-\frac 12} \alpha^2 > 0 \qquad \forall \alpha \in \R \]
			This means that the point is a local minimum.
			
			\item considering instead the last point $x = \frac 1 2$, $y = - \frac{1}{\sqrt 2}$ and $\lambda = e^{-\frac 14}$ we have a similar gradient $\delta h = (1,\sqrt 2)$ that determines a kernel in the form $(\alpha \sqrt 2,-\alpha)$. Evaluating the hessian on the null space base we can see that the matrix is positive defined, in fact
			\[ e^{-\frac 1 4} \begin{pmatrix}
				\alpha \sqrt 2 & -\alpha
			\end{pmatrix} \begin{bmatrix}
				3 & -\sqrt 2 \\ -\sqrt 2 & 2 
			\end{bmatrix} \begin{pmatrix}
				\alpha \sqrt 2 \\ -\alpha
			\end{pmatrix} = 4 e^{-\frac 12} \alpha^2 > 0 \qquad \forall \alpha \in \R \]
		\end{enumerate}
		
		We can see that the both points $\left(\frac 12, \frac 1{\sqrt 2}\right)$ and $\left(\frac 12, -\frac 1{\sqrt 2}\right)$ are local minimum and they are both also global minimum because we can see that $f\left(\frac 12, \frac 1{\sqrt 2}\right) = f\left(\frac 12, - \frac 1{\sqrt 2}\right) = e^{-\frac 1 4}$.
	\end{example}
	
	\subsection{Sylvester theorem}
		The tedious and error prone operation of finding the minimum with the lagrangian multiplier is the one that's performed to determine if the matrix is (or is not) semi positive defined in the kernel $\ker\{\nabla \vett h(\vstar x)\}$ of the gradient of the constraints map. In fact for every stationary point $\vstar x$ of the lagrangian we have to check that
		\[ \vett z^t \, \nabla_{\vett x}^2 \lag(\vstar x,\vstar\lambda) \, \vett z \geq 0 \qquad \forall \vett z\in \ker\{\nabla \vett h(\vstar x)\} \]
		
		For sake of simplicity from now we will denote the matrix $\nabla_{\vett x}^2\lag(\vstar x,\vstar{\lambda})$ as $A$. We can note that the vector $\vett z \in \ker\{\nabla \vett h(\vstar x)\}$ (and from now on we refer to the matrix $\nabla \vett h$ as $B$) can be expressed as a linear combination of the vectors $\vett k_i$ (that are representing the base of $B$) in the way
		\[ \vett z = \vett k_1 \alpha_1 + \vett k_2 \alpha_2 + \dots + \vett k_p \alpha_p = K \vett \alpha \qquad \vett \alpha \in \R^p \]
		We can see that this expression can be reduced to a multiplication of a matrix $K \in \R^{n\times p}$ (whose columns are the vector $\vett k_i$ of the kernel base) and a $p$-dimensional vector $\vett \alpha$ (where $p$ is the number of constraints in the map $\vett h(\vett x)$ ).
		
		We can now rewrite the second order necessary condition considering that
		\[ \vett z^t A \vett z = \vett \alpha^t K^t A K \vett \alpha = \vett \alpha^t C \vett \alpha  \qquad C := K^tAK \in \R^{p\times p} \]
		
		\begin{example}{}
			Let's consider the numerical problem when the values of the matrix $A = \nabla_{\vett x} \lag$ and $B=\nabla \vett h$ are given with values
			\[ A = \matrix{1 & 2 & 3 \\ 2 & 0 & 1 \\ 3 & 1 & 0} \qquad B = \matrix{1&0&0} \]
			
			The first thing now is to compute the manually compute the kernel of the matrix $B$ in $\R^3$ solving the linear system
			\[ \vector{1 &0 &0} \vector{z_1 \\ z_2 \\ z_3} = \vett 0 \qquad \Rightarrow \quad \begin{cases}
				z_1 = 0 \\ z_2 = \alpha \\ z_3 = \beta
			\end{cases} \qquad \alpha,\beta\in\R \]
			At this point we can rewrite the kernel of $B$ using the linear combination of the vector composing the base:
			\[ \ker\{B\} = \vector{0 \\ \alpha \\ \beta} = \vector{0 \\ 1 \\ 0} \alpha + \vector{0 \\ 0 \\ 1} \beta = \underbrace{\matrix{0 & 0 \\ 1 & 0 \\ 0 & 1}}_{=K} \vector{\alpha \\ \beta} \]
			The last thing is now to compute the matrix $C$ that should be analyzed to know if it's (semi) positive defined or not:
			\begin{align*}
				K^t AK& = \matrix{0 & 1 & 0 \\ 0 & 0 & 1} \matrix{1 & 2 & 3 \\ 2 & 0 & 1 \\ 3 & 1 & 0} \matrix{0 & 0 \\ 1 & 0 \\ 0 & 1} \\
				C & = \matrix{0&1\\1&0}
			\end{align*}
			
		\end{example}
	
		This definition reduces the complexity of the problem at analyzing the matrix $C$ (that's smaller than the original matrix $A$) and determining if that particular matrix is (semi) positive defined using two methods:
		\begin{enumerate}
			\item considering the fact that $C$ is a symmetric matrix we know for sure that it can be diagonalised with an expression $T^t \Lambda T$ (where $\Lambda$ is a diagonal matrix containing all the eigenvalues of $C$); considering the expression $\vett \alpha^t T^t \Lambda T \vett \alpha$ in order to have a semi positive defined matrix all the eigenvalue $\lambda_i$ must be positive or at least equals to zero. If it happens that $\lambda_i > 0 \ \forall i$, then the matrix is positive defined and the point is a local minimum;
			
			\item one other solution is to use the \de{Sylvester theorem} that states that\textit{ a symmetric matrix $A\in \R^{n\times n}$ is \textbf{positive defined} if and only if all the \textbf{principal minors} of $A$ are strictly \textbf{positive}} (note that if one minor is equal to zero, no information can be retrieved with this method).
		\end{enumerate} 
		
		\begin{example}{: application of the Sylvester theorem}
			Let's consider now the matrix
			\[ A = \matrix{1 & 2 \\ 2 & 5} \]
			In order to determine if $A$ is positive defined we have to compute all the principle minors starting from the first $A_1$ that's represented only by the element $a_{11}$ of $A$ and so
			\[ A_1 = \det \matrix{1} = 1 > 0 \]
			The second (and last) principle minor of $A$ is the determinant of the whole matrix and it happens that
			\[ A_2 = \det \matrix{1 & 2 \\ 2 & 5} = 1 > 0\]
			Having all minors greater then zero, this means that the matrix $A$ is positive defined. \vspace{3mm}
			
			The same result can be achieved by determining all the eigenvalues of the matrix and showing they are all positive; we can in fact see that the characteristic polynomial of $A$ is equal to
			\[ p(\lambda) = \det\big[A-\lambda I\big] = \det \matrix{1-\lambda & 2 \\ 2 & 5-\lambda} = \lambda^2 - 6\lambda + 1 \]		
			\[ \lambda_{1,2} = \frac{6\pm \sqrt{36-4}}{2} = 3 \pm \frac{\sqrt{32}}{2} \]
			nothing that $\frac{\sqrt{32}}{2} < \frac{\sqrt{36}}{2}	= 3$, then all the eigenvalues $\lambda_1,\lambda_2$ are strictly positive and so $A$ is positive defined.
		\end{example}
		
		\paragraph{Trick for semi positive matrices} The Sylvester theorem gives no hint to determine if a matrix is semi-positive defined when a minor is equal to zero. However a way to determine if the matrix is semi-positive defined is by considering that the matrix $A +\varepsilon I$ should be positive defined for values of $\varepsilon$ approaching zero from the positive direction (for $\varepsilon\rightarrow 0^+$).
		
		\begin{example}{: usage of the Sylvester theorem trick}
			Let's consider now the matrix
			\[ A = \matrix{1 & 2 \\ 2 & 4} \]
			The first minor $A_1 = \det[1] = 1$ is positive, and so we have to compute the second principal minor noting that it's equal to zero:
			\[ A_2 = \det\matrix{1 & 2 \\ 2 & 4} = 4 - 2\cdot 2 = 0 \]
			This relation gives no clue on determining if $A$ is positive defined or not, but considering the trick yet defined we can compute 
			\[ \det\matrix{1+\varepsilon & 2 \\ 2 & 4 + \varepsilon} = (1+ \varepsilon)(4+\varepsilon) - 4 = 5\varepsilon \]
			We can see that this expression, for $\varepsilon \rightarrow 0^+$, is strictly greater then zero and this might conclude our analyses stating that $A$ is semi positive defined. \vspace{3mm}
			
			The same result can be defined by computing the eigenvalues of $A$ and so calculating the roots of the polynomial
			\[ p(\lambda) = \matrix{1-\lambda & 2 \\ 2 & 4-\lambda} = (1-\lambda)(4-\lambda) - 4 = \lambda^2-5\lambda \qquad \Rightarrow \quad \lambda_1 = 0, \lambda_2 = 5\]
			Having one eigenvalues zero (and the other positive) determines that $A$ is semi positive defined.			
		\end{example}
		\begin{example}{: counter example of the Sylvester theorem}
			Let's consider now the matrix
			\[ A = \matrix{ 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 0}  \]
			The first minor $A_1 = \det[1] = 1$ is positive, while the second one is zero, in fact
			\[ A_2 = \det\matrix{1 & 1 \\ 1 & 1} = 0 \]
			This result gives no information but we can use the trick by considering the determinant
			\[ \det\matrix{1 + \varepsilon & 1 \\ 1 & 1 + \varepsilon} = (1+\varepsilon)^2 - 1 = \varepsilon^2 +2\varepsilon > 0 \qquad \textrm{for } \varepsilon > 0  \]
			Being $A$ a $3\times3$ matrix we also need to compute the third minor that leads to another zero:
			\[ A_3 = \det\matrix{ 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 0} = 1 \det\matrix{1&1\\1&1} - 1 \det\matrix{1&1\\1&1} + 0 \det\matrix{1&1\\1&1} = 0  \]
			We can use another time the Sylvester theorem trick considering the determinant
			\[ p(\varepsilon) = \det\matrix{ 1+\varepsilon & 1 & 1 \\ 1 & 1+\varepsilon & 1 \\ 1 & 1 & \varepsilon} = (1+\varepsilon)\big(\varepsilon^2 + \varepsilon - 1\big) - (\varepsilon-1) - \varepsilon = \varepsilon\big(\varepsilon^2 + 2\varepsilon - 2\big) \]
			Having that the derivative $p'(\varepsilon) =3\varepsilon^2 + 4\varepsilon -2$ is negative for $\varepsilon$ approaching zero, then we can conclude that $A$ is not semi positive defined. \vspace{3mm}
			
			The same result can be confirmed calculating the eigenvalues of the matrix and so
			\[ p(\lambda) = \det\matrix{ 1 -\lambda & 1 & 1 \\ 1 & 1 -\lambda & 1 \\ 1 & 1 & -\lambda} = -\lambda\big(\lambda^2 - 2\lambda - 2\big)  \]
			\[ \Rightarrow \qquad \lambda_1 = 0 \qquad \lambda_{2,3} = \frac{2\pm \sqrt{12}}{2} = 1 \pm \sqrt 3 \]
			Noting that $\lambda_3 = 1-\sqrt 3 <0$ is negative, than the matrix $A$ is for sure not semi positive defined.
		\end{example}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	