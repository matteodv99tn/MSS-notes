\chapter{Ordinary Differential Equations and Numerical Solutions}
	To start the description of the \de{differential algebraic equations}\textbf{DAEs} it's firstly necessary to recall what \de{ordinary differential equations} \textbf{ODEs} are, what they mean and how to solve them.
	
	In particular ordinary differential equations is a particular equation that involves a function (for example $y(x)$ depending from the independent variable $x$) and it's derivative as shown in this example:
	\[ y''(x) + x y'(x) + y(x) = \sin x \qquad \leftrightarrow \qquad y'' + xy' + y = \sin x \]
	
	Ordinary differential equations (or system of ODEs) can be written in a \de{\textit{standard form}} made by the \textbf{differential part}, where the first derivative is a function of itself, and the \textbf{initial condition} that set a specified value of the solution of the problem:
	\begin{equation} \label{eq:ode:temp1}
	\begin{cases}
		y' = f\big(x,y(x)\big) = f(x,y) \qquad & \text{: differential part} \\
		y(a) = y_a & \text{: initial condition}
	\end{cases}
	\end{equation}
	Initial conditions are mandatory: the solution of the differential part lonely gives a \textit{family} of solutions (parametric results) whose specific value can be determined only by knowing the value of the function at certain time. Considering the simple case of the differential $x' = 0$ with independent variable $t$, then the general solution is the class of all the constant functions $x(t) = c$ (with $c\in \mathds R$). If a boundary condition is set (example $x(1) = 3$) then we can chose the particular solution (in this case $c = 3$ and so $x(t) = 3$).
	
	Ordinary differential equations can also come in system as in the following example (with $t$ as dependent variable) composed by 2 differential and 2 initial condition terms:
	\[\begin{cases}
		x' = x + y \\ y' = e^x-y \\ x(0) = 0 \\ y(0) = 1
	\end{cases}\]
	
	\paragraph{Vectorial notation} Considering the system of $n=3$ differential equation depending from the independent variable $t$ in the form
	\[ \begin{cases}
		z'(t) = x(t) + z(t) \\ w'(t) = z(t) \\ x'(t) = w(t)z(t) + t(t) \\ x(0) = w(0) = z(0) = 1
	\end{cases} \]
	then it can be rewritten in a vectorial form; considering in fact the substitutions $x(t) = y_1(t)$, $w(t) = y_2(t)$ and $z(t) = y_3(t)$ we have obtain the system
	\[ \begin{cases}
		y_3' = y_1 + y_3 \\
		y_2' = y_3 \\ 
		y_1' = y_2y_3 + t \\ y_1(0) = y_2(0) = y_3(0) = 1
	\end{cases} \qquad \Rightarrow \qquad \begin{cases}
		y_1' = y_2y_3 + t \\
		y_2' = y_3 \\  
		y_3' = y_1 + y_3 \\
		y_1(0) = y_2(0) = y_3(0) = 1
	\end{cases}\]
	Considering the vector $\vett y = (y_1,y_2, y_3)$ we can so rewrite the original system of ODEs as
	\begin{equation}
		\begin{cases}
			\vett y' = \vett f \big(t, \vett y\big) \\ \vett y(0) = \vett 1
		\end{cases}
	\end{equation}
	where
	\[ \vett f : \mathds R^{n+1} \rightarrow \mathds R^n = \begin{pmatrix}
		y_2 y_3 + t \\ y_3 \\ y_1 + y_3
	\end{pmatrix} \]
	In this case the domain of the function $\vett f$ is a vector of dimension $n+1$: $n$ related to the number of ODEs and 1 for the time dependency.
	
	\paragraph{Order of an ODE} The \de{order} of an ordinary differential equation is equal to the maximum order derivative appearing in the differential part of the system; considering as example
	\[ \begin{cases}
		y'' + y' + z' = 0 \\ z' + t = 0
	\end{cases} \]
	the order of such ordinary differential system is equal to 2 (associated to the term $y''$). 
	
	In general numerical methods are defined for 1$^{st}$ order ODEs and so it's necessary (but most importantly possible) to convert any generic differential equation into a system of 1$^{st}$ order ODEs by performing a change of variable.
	
	\begin{example}{: reduction to a system of ODEs of first order}
		Given the system of ODEs of the 3$^{rd}$ order in the independent variable $t$ defined as
		\[ \begin{cases}
			x''' + y' = x^2 + t \\ 
			y'' + x = t^2 + 1 \\
			x(0) = 0 \qquad y(0) = 0 \\
			x'(0) = 0 \qquad y'(0) = 2 \\
			x''(0) = 2
		\end{cases} \]
		the reduction to a system of first order ODEs is made by introducing the variable $z = x'$; defining instead the function $x'' = z' = w$ we also have that $z' = w$ hence $x''' =z'' = w'$; finally we can set $y' = p$ hence $y'' = p$. The system so becomes
		\[ \begin{cases}
			w' + p = x^2 + t \\ 
			p' + x = t^2 + 1 \\
			x' = z \\ 
			z' = w \\
			y' = p \\
			x(0) = 0 \qquad y(0) = 0 \\
			z(0) = 0 \qquad p(0) = 2 \\
			w(0) = 2
		\end{cases} \]
		This system present 3 more differential terms and so seems \textit{more difficult}, however this formulation is numerically more suitable for the computation.
	\end{example}
	
\section{Existence of the solution}
	Given the general system of $n$ ordinary differential equations in the standard form
	\[ \begin{cases}
		\vett y' = \vett f\big(t,\vett y\big) \\ \vett y(a) = \vett y_a
	\end{cases} \]
	using \textbf{Peano's theorem} we can state that if the vectorial map $\vett f: \mathds R^{n+1} \rightarrow \R^n$ is continuous, then a solution exists in the neighbourhood of the point $(t=a,\vett y = \vett y_a)$. This theorem provide a sufficient condition to determine if a solution exists, but it doesn't state that the solution is unique.
	
	\begin{example}{: ODE with multiple solution} \label{ex:ode:multisol}
		Considering the ordinary differential equation in the independent variable $t$
		\[ \begin{cases}
			y' = \sqrt{|y|} \\ y(0) = 0
		\end{cases} \]
		we can see that the map $f(y) = \sqrt{|y|}$ is continuous for all $y\in \R$, hence for Peano's theorem a solution must exists for $t$ \textit{sufficiently close} to $0$. In particular we can observe that the function
		\[y(t) = 0 \]
		is a solution of the system, in fact it matches the initial condition and we have that it's derivative $y' = \frac{dy}{dt} = 0$ is equal to the function $f(y) = \sqrt{|0|} = 0$.
		
		However this is not the lonely solution, considering in fact the function
		\[ y(t) = \frac{t^2}{4} \textrm{sign}(t) = \begin{cases}
			\frac{t^2}4 \qquad & t \geq 0 \\
			-\frac{t^2}4 & t < 0
		\end{cases} \]
		Observing that the derivative of such function can be regarded as
		\[ y'(t) = \begin{cases}
			\frac{t}4 \qquad & t \geq 0 \\
			-\frac{t}2 & t < 0
		\end{cases} \qquad \Rightarrow \quad y'(t) = \frac{|t|}{2} \]
		we can also check that the provided solution solves the differential part of the system, in fact
		\[ \sqrt{|y(t)|} = \sqrt{\left| \frac{t^2}{4} \textrm{sign}(t) \right|} = \sqrt{\frac{t^2}{4}} = \frac{|t|}{2} = y'(t) \]
	\end{example}
	In general when solving system of differential equation we want to be sure that the solution exists (using as example Peano's theorem) but that is also unique. In order to do so we have to defined the \textbf{Lipschitz continuity}:
	\begin{equation} \label{eq:ode:lipschitz}
		f:\R^n\rightarrow \R \textrm{ is Lip. cont.} \quad \textrm{if } \exists L \in \mathds R \quad \textrm{such that} \quad \big\| f(\vett x) - f(\vett y) \big\| \leq L\big\|\vett x - \vett y\big\| \qquad \forall \vett x,\vett y \in \mathds R 
	\end{equation}
	We can so state that a system of ordinary differential equation in the standard form has one solution if the map $\vett f:\R^{n+1}\rightarrow \R^n$ is continuous (from Peano) and is also lipschitzian.
	
	\begin{example}{: Lipschitz continuity}
		Considering the problem of example \ref{ex:ode:multisol} we can prove that the system has multiple solution by checking that's not Lipschitz continuous. Known that the map $f(t,y):= \sqrt{|y|}$ is continuous we can apply Lipschitz definition in the particular case when one point is the origin of the the axis:
		\begin{align*}
			\big| f(0,y) - f(0,0)\big| & \leq L\big|x-0\big| \\
			\left| \sqrt{|y|} - \sqrt{|0|} \right| & L  \leq |y| \\
			\cancel{\sqrt{|y|}} & \leq L \cancel{\sqrt{|y|}} \sqrt{|y|} \\
			L & \geq \frac 1{\sqrt{|y|}}
		\end{align*}
		Observing that for $y\rightarrow 0$ the denominator converges to zero this means that $L$ must diverge to $\infty$ meaning that the function is not lipschitzian: the solution exists (Peano's theorem) but it might not unique (for Lipschitz).
	\end{example}

\section{Taylor expansion}
	The main tool used for numerical approximate solution of (system of) ordinary differential equation is the \de{Taylor series expansion} that's used to approximate a function $f:\R\rightarrow \R$ of class $\mathcal C^\infty$ as a polynomial in the neighbourhood of a specific point $x_0$; given $h$ as the deviation from the point $x_0$ for \textit{small} values of $h$ we can approximate the function
	\begin{equation} \label{eq:ode:taylorgeneric}
	\begin{split}
		f\big(x_0 + h\big) & \approx a_0 + a_1 h + a_2 h^2 + a_3 h^3 + \dots + a_n h^n \\
		& \approx a_0 + \sum_{k=1}^\infty a_k h^k
	\end{split} 
	\end{equation}
	
	Evaluating both members allows to obtain the first coefficient $a_0 =f(x_0)$ of the Taylor expansion, in fact
	\[ f\big(x_0\big) = a_0 + \sum_{k=1}^\infty a_k 0^k = a_0  \]
	By differentiation we can also obtain other coefficients
	\begin{align*}
		f'(x_0+h) & = a_1 + \sum_{k=2}^\infty a_k h^{k-1}k \qquad && \xrightarrow{h = 0} \qquad a_1 = f'(x_0) \\
		f''(x_0+h) & = 2a_2 + \sum_{k=3}^\infty a_k h^{k-2}k(k-1) \qquad && \xrightarrow{h = 0} \qquad 2a_2 = f''(x_0) \\
	\end{align*}
	Considering that in general each coefficient can be computed as $a_k = f^{(k)}(x_0) / k!$ we can better rewrite the Taylor series expansion of equation \ref{eq:ode:taylorgeneric} as
	\begin{equation} \label{eq:ode:taylor}
		f\big(x_0 + h\big) \approx \sum_{k=0}^{\infty} \frac{f^{(k)}(x_0)}{k!} h^k
	\end{equation}
	
	\paragraph{Truncation} From a numerical standpoint the computation of the Taylor series is truncated to a order $n$ and so we can use the formulation
	\begin{equation}
		f\big(x_0 + h\big) = \sum_{k=0}^{n} \frac{f^{(k)}(x_0)}{k!} h^k + R_n(h)
	\end{equation}
	where $R_n$ is the reminder due to the truncation of the series that can be evaluated in multiple ways:
	\begin{itemize}
		\item using Peano's formulation the more formal definition of the reminder is
		\[ R_n(h) = \int_{x_0}^{x_0+h} f^{(n+1)} (s) \frac{(s-h)^n}{n!}\, ds\]
		This formulation is still complex and numerically \textit{unusable};
		\item the Lagrange reminder in the form
		\[ R_n(h) = f^{(n+1)}(\zeta) \frac{h^{n+1}}{(n+1)!} \qquad \textrm{with } \zeta \in \big( x_0,x_0+h \big) \]
		\item the \textit{big O} notation $R_n(h) = \mathcal O\big(h^{n+1}\big)$; in particular we denote $g(x) = \mathcal O\big(f(x)\big)$ if 
		\[ \exists C \in \R \quad \textrm{such that} \quad |g(x)| \leq C|f(x)| \]
		\item the \textit{small o} notation $R_n(h) = o\big(h^n\big)$; we say that $g(x) = o\big(f(x)\big)$ if $f$ is lipschitzian (equation \ref{eq:ode:lipschitz}) and we have that
		\[ \lim_{x\rightarrow 0} \frac{o\big(f(x)\big)}{g(x)} = 0 \]
	\end{itemize}
	
	\paragraph{Common Taylor expansions}Examples of notable series expansion on the point $x_0 = 0$ for well-known functions are the exponential, the cosine and sine:
	\begin{equation}
	\begin{split}
		e^h & = \sum_{k=0}^\infty \frac{h^k}{k!} = 1 + h + \frac{h^2}{2} + \frac{h^3}{3!} + \frac{h^4}{4!} + \dots \\
		\cos h & = \sum_{k=0}^{\infty} -1^{k} \frac{h^{2k}}{2k!} = 1 - \frac{h^2}{2!} + \frac{h^4}{4!} - \frac{h^6}{6!} + \dots \\
		\sin h & = \sum_{k=0}^{\infty} -1^{k} \frac{h^{2k+1}}{(2k+1)!} = h - \frac{h^3}{3!} + \frac{h^5}{5!} - \frac{h^7}{7!} + \dots
	\end{split}
	\end{equation}
	
	\paragraph{Existence of the expansion} Considering the definition of the lagrangian reminder in the form $f^{(m)}(\zeta) \frac{h^m}{m!}$ if we truncate the series to higher order (bigger value of $m$) we observe that for $h <1$ the term $h^m/m!$ tends to zero, and so if we ensure that the $m$-th derivative doesn't diverge we have that the Taylor series converge to the \textit{real} function. However this sometimes can fail: considering as example the function
	\[ f(x) = \begin{cases}
		e^{-\frac 1 {x^2}} \qquad & x> 0 \\ 0 & \textrm{otherwise}
	\end{cases} \]
	it's proven that the function is \textit{smooth} ($f\in C^\infty$) and that the $k$-th derivative is in the form
	\[ \left(e^{-\frac 1 {x^2}}\right)^{(k)} = e^{-\frac 1 {x^2}} \frac{p_1(x)}{p_2(x)} \hspace{2cm} p_1,p_2 \textrm{ polynomials} \]
	and converges to zero for $x\rightarrow 0$. By applying the definition of the Taylor expansion in $x_0=0$ we so have that
	\[ f(0+h) = \sum_{k=0}^\infty f^{(k)}(0) \frac{h^k}{k!} = 0 \]
	This expansion correctly models the left-hand side of the function $f$ but not the right side, and so the Taylor expansion fails.
	
\subsection{Multi-variable functions}
	Until now we have defined the Taylor expansion of function with one variable in the form $f(x)$, but such concept should be extended to function of multiple variables. Considering the simple case of $f:\R^2\rightarrow \R$ in order to perform the Taylor series we have to ensure a continuity up to an order $m$ for the function, meaning 
	\begin{equation}
		f(x,y) \in C^m \hspace{1.4cm} \Leftrightarrow \hspace{1.4cm} \frac{\partial^{i+j}}{\partial x^i \partial y^j} f(x,y) \in C \quad \forall i+j \leq m
	\end{equation}
	
	We have the formal definition of the Taylor series for functions of one variable (equation \ref{eq:ode:taylor}) and so, as idea, we can \textit{slice} the function $f$ passing through a point $(x_0,y_0)$ with a direction $(x-x_0,y-y_0) = (d_x,d_y)$ using a function
	\[ g(t) = f(x_0 + td_x,y_0 + td_y) \]
	The idea is so to compute the Taylor series of this function in the neighbourhood of $t = 0$:
	\[ g(t) = g(0) + g'(t) t + g''(0) \frac{t^2}{2!} + g'''(0) \frac{t^3}{3!} +  \dots \]
	The term $g'(t)$relates to the total derivative of $f(x_0 + t d_x, y_0 + t d_y)$ respect to the variable $t$, meaning that
	\begin{align*}
		g'(t) & = \frac{d}{dt} f\big(x_0 + t\, d_x, y_0 + t\, d_y\big) \\
		& = \pd {f(\dots)} x \frac{d(x_0+t\, d_x)}{dt} + \pd {f(\dots)} y \frac{d(y_0+t\, d_y)}{dt} \\
		& = \pd{f(\dots)}{x} d_x + \pd{f(\dots)}{y} d_y = \pd{f(\dots)}{x} (x-x_0) + \pd{f(\dots)}{y} (y-y_0) \\
		g'(0) & = \pd {f(x_0,y_0)}{x}(x-x_0) + \pd {f(x_0,y_0)}{y}(y-y_0)
	\end{align*}
	Using a similar methodology it's possible to compute the second order derivative of $g$ as
	\begin{align*}
		g''(t) & = \frac d{dt}g'(t) = \frac{d}{dt} \left( \pd{f(\dots)}{x} (x-x_0) + \pd{f(\dots)}{y} (y-y_0) \right) \\
		& = \frac{\partial^2 f(\dots)}{\partial x^2} (x-x_0)^2 + \frac{\partial^2 f(\dots)}{\partial y^2} (y-y_0)^2 + 2 \frac{\partial^2 f(\dots)}{\partial x \, \partial y} (x-x_0)(y-y_0)
	\end{align*}
	
	Considering as more general statement to express the Taylor series respect to a point $(x_0,y_0)$ moving with values $h = td_x$ and $k = t d_y$ the series truncated to the second order is so
	\begin{equation}
	\begin{split}
		f(x_0 + h, y_0 + k) = & f(x_0,y_0)  + \left. \pd f x\right|_{(x_0,y_0)} h + \left. \pd f y\right|_{(x_0,y_0)} k  \\ & + \frac 1 2 \left.  \frac{\partial^2 f}{\partial x^2} \right|_{(x_0,y_0)} h^2 + \frac 1 2 \left.  \frac{\partial^2 f}{\partial y^2} \right|_{(x_0,y_0)} k^2 + \left.  \frac{\partial^2 f}{\partial x \,\partial y} \right|_{(x_0,y_0)} hk
	\end{split}
	\end{equation}
	This representation can be compacted using a vectorial/matrix notation condensing $(x_0,y_0)$ in the vector $\vett x_0$ and the increment $(h,k) = \vett h$ we can define
	\begin{equation}
		f (\vett x_0 + \vett h) = f(\vett x_0) + \nabla f(\vett x_0)\, \vett h + \frac 12 \vett h^t \, \nabla^2 f(\vett x_0)\, \vett h + R_3(\|\vett h\|)
	\end{equation}
	where $\nabla f(\vett x) = (\pd f {x_1}, \dots, \pd f {x_n})$ is the \textbf{gradient} of the function (and is a row vector) and $\nabla^2f(\vett x)$ is the \textbf{hessian matrix} of $f$. Note that this formulation is general and is valid for any multi-variable function $f:\R^n\rightarrow \R$ assuming that's at least $C^2$.
	
	\paragraph{Higher order Taylor expansion} In order to perform Taylor expansion with order greater than 2 it's necessary to use a \textbf{tensor} notation; in particular the expansion of a function $f:\R^n\rightarrow \R$ up to the order $m$ is described by the equation
	\begin{equation}
		f\big(\vett x_0 + \vett h\big) = \sum_{k=0}^{m} \sum_{|\alpha|=k} \partial_\alpha f(\vett x_0) \frac{\vett h^\alpha}{\alpha!} + R_m
	\end{equation}
	where $\alpha = (\alpha_1,\alpha_2,\dots,\alpha_n)$ is the multi-index vector that consists of non-negative integers; the condition $|\alpha|=k$ based on the norm $|\alpha| = \alpha_1+\alpha_2+\dots + \alpha_n$ choses all combination of $\alpha$ satisfying such relation and the multi-index variable is used for the computation following the expressions
	\[ \partial_\alpha := \partial_{x_1}^{\alpha_1} \partial_{x_2}^{\alpha_2} \dots \partial_{x_n}^{\alpha_n} f\vett(x) \hspace{2cm} \vett h^\alpha := h_1^{\alpha_1} h_2^{\alpha_2} \dots h_n^{\alpha_n} \]
	
\section{Numerical methods}
\subsection{Taylor series based}
	In practise (systems of) ordinary  differential equations are numerically solved by computes using algorithms some of which are based on the Taylor series expansion. Considering for the simplicity a first order ordinary differential equation in the standard form (equation \ref{eq:ode:temp1})
	\[ \begin{cases}
		y' = f(x,y) \\ y(a) = y_a
	\end{cases} \]
	If we consider $y(x)$ the solution of the ODE system, that can be expanded up to the second order with Taylor as
	\[ y(x+h) = y(x) + y'(x) h + \mathcal O (h^2) = y(x) + f(x,y) h + \mathcal O (h^2) \]
	The most basic idea to numerically compute the solution is to subdivide the interval $[a,b]$ of integration into $N$ pieces having each a width $h = \frac{b-a}{N}$; this discretization of the $x$ axis determines so the sequence of point $x_k = a + h k$. With this idea in mind we can see that the yet computed Taylor expansion can be regarded as
	\begin{equation} \label{eq:ode:temp2}
		y(x_{k+1}) = y(x_k) + f\big(x_k, y(x_k)\big) h + \mathcal O(h^2)
	\end{equation}
	Numerical methods determines a sequence of output $y_k$ that tends to approximate the real behaviour of the solution, hence $y_k \approx y(x_k)$. The simplest numerical method to solve the ordinary differential equation is by simply using  equation \ref{eq:ode:temp2} neglecting the reminder:
	\begin{equation} \label{eq:ode:expliciteuler}
		y_{k+1} = y_k + f(x_k,y_k) h 
	\end{equation}
	
	\paragraph{Error} Numerical methods are approximation of the analytical solutions, hence intrinsically contains an error that should be somehow defined in order to determine \textit{how bad} or \textit{good} the numerical solution is. If we new define the error $\epsilon_k$ on the $k$-th step as the difference between the analytical and numerical solution
	\begin{equation} \label{eq:ode:temp3}
		\varepsilon_{k+1}  = y(x_{k+1}) - y_{k+1} = y(x_k) - y_k + \Big( f\big(x_k, y(x_k)\big) - f(x_k,y_k) \Big) h + \frac{y''(\zeta_k)}{2} h^2 
	\end{equation}
	where the reminder $\mathcal O(h^2)$ as been substituted with the lagrangian one and hence $\zeta_k \in (x_k,x_{k+1})$. Considering that the function $f$ is assumed to be lipschitzian, then it means that exists $L\in \R$ such that
	\[ \left| f\big(x_k,y(x_k)\big) - f(x_k,y_k)\right| \leq L \left| y(x_k) - y_k \right|\]
	Considering this inequality, knowing that $y(x_k) - y_k = \epsilon_k$ and using the triangular inequality we can rewrite equation \ref{eq:ode:temp3} as
	\begin{equation}
		|\varepsilon_{k+1}| = |\varepsilon_k| + h L \big|y(x_k)-y_k\big| + \frac{h^2}{2} |y''(\zeta_k)| = A |\varepsilon_k| + B
	\end{equation}
	where $A = 1 + hL$ and $B = \frac{h^2}{2} M_2$. In particular $M_2$ is the constant that bounds the second derivative of $y$ in the domain of integration, meaning
	\[ M_2 = \sup_{x\in[a,b]} \{ y''(x) \} \]
	Starting with the theoretical assumption that $\varepsilon_0 = 0$ (the initial error is null given the initial condition) and observing that $A\rightarrow 1$ and $B\rightarrow 0$, then we have that $|\varepsilon_1| \leq A 0 + B = B$; the sequent error is so $|\varepsilon_2| \leq A |\varepsilon_1| + B = B(1+A)$. Computing $|\varepsilon_3| \leq B(1+A+A^2)$ it's possible to prove by induction that the error has a formulation
	\[ |\varepsilon_k| \leq \big(1 + A + A^2 + \dots + A^{k-1}\big) B \]
	
	The maximum error $E_h$ of this numerical method is so determined by considering the maximum error respect to all discretization steps:
	\[ E_h = \max_{k=0,\dots,N} |\varepsilon_k| \leq \max_{k=0,\dots,N} \big(1 + A + A^2 + \dots + A^{k-1}\big) B = \big(1 + A + A^2 + \dots + A^{N-1}\big) B\]
	Considering the geometrical series determined by $A+ A^2+\dots$ we obtain that such sequence sums to the value $\frac{1-A^N}{1-A}$ and so the error can be considered as
	\[ E_h \leq \frac{A^N-1}{A-1} B = \frac{A^N - 1}{1+hL - 1} \frac{h^2}{2}M_2 = \frac{A^h - 1}{L} \frac{h}2 M_2 \]
	All we need now is to quantity the error related to the  term $A^N$; considering that the Taylor series of the exponential sequence $e^x = 1 + x + \frac{x^2}{2!}+\dots$ we have that such quantity is always greater than $1+x$, and so knowing that $A= 1+hL$ we have that $(1+hL)^N \leq \big(e^{hL}\big)^n = e^{LNh}$. Observing that $Nh=b-a$ we can finally state the total error as
	\begin{equation}
		E_h\leq \frac{e^{L(b-a)} M_2}{2L} h = Ch
	\end{equation}
	where $C\in \R$ is a constant, meaning that for $h\rightarrow 0$ the error presents the expected behaviour of approaching zero. By a computation point of view this method isn't that good, because in order to halve the error we have to halve also the integration step $n$ (doubling the number of intervals $N$). If we would have considered other methods truncated to higher orders of derivation what we would have obtained is an error in the form
	\[ E_h = C h^p \]
	hence by dividing by 2 $h$ the error would have been reduces by $2^p$.
	
	\paragraph{System of ODEs} Considering the more general case of a system of ordinary differential equation in the standard form using the vectorial notation
	\[ \begin{cases}
		\vett y' = \vett f\big(t,\vett y\big) \\ \vett y(a) = \vett y_a
	\end{cases} \]	
	the yet described method can be still used by expanding each component of $\vett y$, meaning that the numerical solution can be approximated as
	\[ \vett y_{k+1} = \vett y_k + h \vett f(x_k, \vett y_k) \]
	and the error can be regarded as $E_h = \max \|\vett y(x_k) - \vett y_k\| \leq Ch$.
	
	\subsubsection{Implicit method: back-backward Euler }
		The numerical method described until now is the \textbf{explicit Euler integration} for determining the solution of ordinary differential equations; the formulation as provided in equation \ref{eq:ode:expliciteuler} (page \pageref{eq:ode:expliciteuler}) is computationally lightweight (because by knowing $x_k,y_k$ at the current stage allows to automatically compute $y_{k+1}$), however is unstable and can quickly diverges from the analytical solution.
		
		A way to solve such problematic is by using \textbf{implicit method} that are constructed by performing the Taylor expansion \textit{from the left}. Considering as example the \textbf{Euler back-backward} method, the computed Taylor series is
		\[ y(x-h) = y(x) - h y'(x) + \frac{h^2}{2} y''(\zeta) = y(x) - h f(x,y) + \frac{h^2}{2} y''(\zeta) \] 
		This gives origin to the iterative numerical method defined as
		\begin{equation} \label{eq:ode:impliciteuler}
			y_{k-1} = y_k - h f(x_k,y_k)
		\end{equation} 
		where the solution of the current output $y_k$ is implicitly defined as function of the current \textit{position} $x_k$ and the previous value $y_{k-1}$. This formulation increases the computational complexity (at each iteration a non-linear system has to be solved in order to determine the implicit solution $y_k$) but strongly increases the robustness of the algorithm.
	
	\subsubsection{Other methods based on the Taylor series}
		In general given the ordinary differential equation
		\[ \begin{cases}
			y' = f(x,y) \\ y(a) = y_a
		\end{cases} \]
		in order to have a solution we assume that $f$ is continuous and lipschitzian; given $y(x)$ the exact solution of the problem, we can apply the Taylor expansion on such result obtaining
		\[ y(x+h) = y(x) + h y'(x) + \frac{h^2}{2} y''(x) + \dots + \frac{h^p}{p!} y^{(p)} (x) + \mathcal O(h^{p+1}) \]
		Knowing that $y(x)$ is the solution of the ODE, then we have that $y'(x) = f\big(x,y(x)\big)$; considering now so it's derivative we have that
		\begin{align*}
			y''(x) & = \frac d{dx} y'(x)  = \frac d{dx} f\big(x,y(x)\big) = \pd f x \big(x,y(x)\big) + \pd f y\big(x,y(x)\big) y'(x) \\
			& = \pd f x \big(x,y(x)\big) + \pd f y\big(x,y(x)\big) f\big(x,y(x)\big)
		\end{align*}
		This allows to rewrite the Taylor expansion as
		\[ y(x+h) = y(x) + h y'(x) + \frac{h^2}{2} \left(\pd f x + \pd f y f \right) + \dots + \frac{h^p}{p!} y^{(p)} (x) + \mathcal O(h^{p+1}) \]
		The previously described explicit Euler method was determined by neglecting the terms with order higher then $h$, but in this case we have the possibility to express also $y''$ as function of $f$ and $x,y$ increasing hence the numerical accuracy. The numerical method is so
		\begin{equation} \label{eq:ode:eulersecond}
			y_{k+1} = y_k + h f(x_k,y_k) + \frac h 2 \left( \pd{f(x_k,y_k)}{x} + \pd{f(x_k,y_k)}{y} f(x_k,y_k) \right)
		\end{equation}
		where so in this case we dropped an error in the form $\mathcal O(h^3)$. Increasing the order of the numerical method increases the solution but requires the symbolical computation of the derivatives $\pd f x,\pd f y$; we can carry on the process to explicitly determine all the derivative $y^{(p)}$ (up to a certain order) as function of $x,y$ and partial derivatives of $f$. Considering as example the third derivative of $y$ we see that
		\begin{align*}
			y'''(x) & = \frac{d}{dx} y''(x) \\ & 
			= \pd{^2 f}{x^2} + \pd{^2f}{x\, \partial y} y' + \pd{^2 f}{x\,\partial y} f + \pd{^2f}{y^2} y f + \pd{f}{y} \left( \pd f x + \pd f y y'\right)
		\end{align*}
		We can see that numerical method based on the Taylor series are conceptually easy but requires \textit{a lot} of symbolical computation in order to express the derivatives as function of the known variables.
		\begin{example}{: explicit Euler scheme of the second order}
			Given the ODE
			\[ \begin{cases}
				y' = xy + x \\ y(0) = 1 
			\end{cases} \]
			to express the 2$^{nd}$ order Euler scheme we need to compute the partial derivatives
			\[ \pd f x = y + 1 \hspace{2cm} \pd f y = x \]
			and hence using equation \ref{eq:ode:eulersecond} we obtain the method
			\[ y_{k+1} = y_k + h \big(x_k y_k + x_k\big)  +\frac {h^2}2 \Big[ y_k + 1 + x_n \big(x_ny_n + x_n\big) \Big] \]
			Higher order methods for this problem can be implemented explicitly computing the derivatives
			\begin{align*}
				y'(x) & = xy(x) + x  \\
				y''(x) & = y(x) + x y'(x) + 1 \\
				y'''(x) & = y'(x) + y'(x) + x y''(x) = 2 y'(x) + x y''(x) \\
				y^{(4)}(x) & = 2 y''(x) + y''(x) + xy'''(x) = 3 y''(x) + x y'''(x) \\
				& \hspace{2.2mm}  \vdots
			\end{align*}
			The idea is so to determine the numerical method as
			\[ y_{k+1} = y_k + h y_k' + \frac {h^2}2 y''_k + \frac{h^3}{6} y'''_k + \frac{h^4}{24} y_k^{(4)} + \dots \]
			where
			\[ y'_k = x_k y_k + x_k \qquad \rightarrow \quad y''_k = y_k +x_k y'_k +1 \] \[ \rightarrow \quad y'''_k = 2y'_k + x_k y''_k \qquad \rightarrow \quad y^{(4)}_k = 3 y_k'' + x y'''_k \qquad \rightarrow \quad \dots \]
		\end{example}
	
		\paragraph{Economic Taylor scheme} Recalling the higher order method shown in the previous example, a way to simplify the notation on the numerical method based on the Taylor series we consider the scheme
		\[ y_{k+1} = y_k + h y'_k + \frac{h^2}2 y''_k + \dots + \frac{h^p}{p!} y_k^{(p)} \]
		where the numerical derivatives are recursively defined considering that $y_k' = f(x_k,y_k)$:
		\begin{align*}
			y'_k = D_1\big(x,y(x) \big) & = f(x,y) \\
			y''_k = D_2\big(x,y(x), y'(x) \big) & = \pd {D_1} x (x,y) + \pd {D_1} y (x,y)\, y'(x,y) \\
			y'''_k = D_3\big(x,y(x), y'(x),y''(x) \big) & = \pd {D_2} x (x,y) + \pd {D_2} y y' + \pd {D_2}{y'}y''\\
			& \hspace{2mm} \vdots \\
			y^{(p)}_k = D_p\big(x,y(x),y'(x),\dots, y^{(p-1)}(x)\big) & = \pd{D_{p-1}}{x} + \pd{D_{p-1}}y + \dots + \pd{D_{p-1}}{y^{(p-2)}} y^{(p-1)}
		\end{align*}
			
	\subsection{Runge-Kutta}
		Considering the statements seen for numerical methods derived from the Taylor series expansion, we can see that the function $f$ can be expanded by parameters $\alpha,\beta$ as
		\[ f(x+\alpha, y+\beta) = f(x,y) + \pd f x \alpha + \pd f y \beta + \frac 1 2 \pd{^2f}{x^2}\alpha^2 + \pd{^2f}{x\, \partial y} \alpha \beta + \frac 1 2 \pd{^2f}{y^2} \beta^2 + \mathcal O \Big(\sqrt{\alpha^2+\beta^2}^3 \Big)\]
		The idea that originates the Runge-Kutta method is so to combine many evaluation of $f(x+\alpha,y+\beta)$ in order to match the results provided by the Taylor series; in general this match determines an error that however should be at maximum comparable to the order $\mathcal O(x^n)$ required. Considering the expansion previously discussed
		\[ y(x+h) = y(x) + h f\big(x,y(x)\big) + \frac{h^2}{2} \left(\pd f x + \pd f y f \right) + \mathcal O(h^3)  \]
		we can consider the term
		\[ (i): \hspace{2cm} h\, f\big(x,y(x)\big) + \frac{h^2} 2 \left( \pd f x + \pd f y f\big(x,y(x)\big) \right) \]
		and it's Taylor expansion
		\begin{align*}
			h\omega f\big(x,y(x)\big) + h\gamma f\big(x+\alpha h, y(x) + \beta h\big) & = h\omega f + h\gamma \left( f + \pd f x \alpha h + \pd f y \beta h + \mathcal O(h^2) \right) \\
			(ii): \hspace{2cm} & = h\omega f + h\gamma f + h^2 \gamma \alpha \pd f x + h^2 \gamma \beta \pd f y + \mathcal O(h^3)
		\end{align*}
		By now subtracting $(ii)$ to $(i)$ we determine
		\[ h f \big(1-\omega - \gamma \big) + h^2 \pd f x \left( \frac 1 2 - \gamma \alpha \right) + h^2 \pd f y \left( \frac f 2 - \beta \gamma \right) + \mathcal O(h^3) \]
		In order to reduce the error with a threshold $\mathcal O(h^3)$ we have to set to zero all the multiplicative terms of $f$ (and it's derivatives) determining the following non-linear system:
		\[ \begin{cases}
			1 - \omega - \gamma = 0 \\ \frac 1 2 - \gamma \alpha = 0 \\ \frac f 2 - \beta \gamma = 0
		\end{cases} \]
		If we determine parameters $\omega,\gamma,\alpha,\beta$ that satisfy such conditions we have that the expansion
		\[ y(x+h) = y(x) + \omega \, f\big(x,y(x)\big) + \gamma\, f\big(x+ \alpha h, y(x)+ \beta h \big) + \mathcal O(h^3) \]
		matches the result obtained with Taylor; the system has 3 equation but the unknowns are 4, having so the a parametric solution in the form $\omega = 1-\gamma$, $\alpha = \frac 1 {2\gamma}$ and $\beta = \frac{f}{2\gamma}$ is always satisfied. Substituting this in the original expression we have that all the 2$^{nd}$ order numerical method that do not use \textit{explicitly} the partial derivatives of $f(x,y)$ are
		\[ y(x+h) = y(x) + h (1-\gamma) f\big(x,y(x)\big) + h \gamma f\left( x + \frac{h}{2\gamma}, y(x) + \frac{h}{2\gamma}f\big(x,y(x)\big) \right) + \mathcal O(h^3) \]
		determining the numerical method
		\begin{equation}
			y_{k+1} = y_k + h (1-\gamma) f\big(x_k,y_k\big) + h \gamma f\left( x_k + \frac{h}{2\gamma}, y_k + \frac{h}{2\gamma}f\big(x_k,y_k\big) \right)
		\end{equation}
	
		\paragraph{Definition} The idea of the \de{Runge-Kutta} method is use a combination of \textit{displacements} in order to match \textit{as much as possible} the Taylor expansion of the exact solution; in particular the numerical steps are written as
		\begin{equation} \label{eq:ode:rungekutta}
			\vett y_{k+1} = \vett y_k + \sum_{i=1}^{s} b_j \vett k_j
		\end{equation}
		where the $s$ vectors $\vett k_j$ (where $s$ is the order of the Runge-Kutta method) are obtained as
		\begin{equation}
		\begin{cases}
			\vett k_1 = h \, \vett f\left( x_k + c_1 h, y_k + \sum_{j=1}^s A_{1j} \vett k_j \right) \\
			\vett k_2 = h \, \vett f\left( x_k + c_2 h, y_k + \sum_{j=1}^s A_{2j} \vett k_j \right) \\
			\hspace{5.7mm} \vdots \\			
			\vett k_s = h \, \vett f\left( x_k + c_s h, y_k + \sum_{j=1}^s A_{sj} \vett k_j \right) \\
		\end{cases}
		\end{equation}
		where the coefficients $c_j,b_j, A_{ij}$ are computed in such a way that $y_{k+1} - y(x_{k+1}) = \mathcal O(h^p)$ (so the error between the computed value and the theoretical solution) where $p$ is as large as possible. Such values are already tabled in the \de{\textit{Runge-Kutta tableaus}} represented as
		\begin{equation}
		\begin{array}{c | c c c}
			& \\
			\vett c & & A \\ 
			& \\ \hline  & & \vett b^t & 
		\end{array}
		\end{equation}
		where $\vett c = (c_1, \dots, c_s)$, $\vett b = (b_1,\dots, b_s)$ (with $\vett c,\vett b \in \R^s$) and $A\in \R^{s\times s}$.
		
		\paragraph{Runge-Kutta of order 4} A tableau for the Runge-Kutta method of order 4 is defined as
		\[ \begin{array}{c | c c c c}
			0 &  \\
			\frac 1 3 & \frac 1 3 & \\
			\frac 2 3 & - \frac 1 3 & - 1 \\
			1 & 1 & -1  & 1 \\ \hline
			& \frac 1 8 &  \frac 3 8 & \frac 3 8 & \frac 1 8
		\end{array} \]
		where the non-represented terms are zeros. Recalling equation \ref{eq:ode:rungekutta} as the definition of the Runge-Kutta, we have that the numerical method derived from that is
		\[ y_{k+1} = y_j + \frac 1 8 k_1 + \frac 3 8 k_2 + \frac 3 8 k_3 + \frac 1 8 k_4 \]
		where
		\[ \begin{cases}
			k_1 & = h\, f(x_k,y_k) \\
			k_2 & = h\, f\left( x_k + \frac 1 3 h, y_k + \frac 1 3 k_1 \right) \\
			k_3 & = h\, f\left( x_k + \frac 2 3 h, y_k - \frac 1 3 k_1 - k_2\right) \\
			k_4 & = h\, f\left( x_k + h, y_k + k_1 - k_2 + k_3 \right) \\
		\end{cases}\]
		In this case the method is \textbf{explicit}: in fact we can sequentially compute $k_1$ (that depends on the knowns $h,x_k,y_k$) and sequentially $k_2$, then $k_3$ and lastly $k_4$.	
	
		\paragraph{Euler methods} If we consider the \textit{strange} tableau 
		\[ \begin{array} {c |c}
			0 & 0 \\ \hline & 1
		\end{array} \]
		what we obtain is the explicit Euler method, in fact having $k_1 = h\, f(x_k,y_k)$ determines the method
		\[ y_{k+1} = y_k + 1 k_1 = y_k + h f(x_k,y_k) \]
		and perfectly matches the definition provided in equation \ref{eq:ode:expliciteuler} at page \pageref{eq:ode:expliciteuler}. Considering now instead the tableau
		\[ \begin{array} {c |c}
			1 & 1 \\ \hline & 1
		\end{array} \]
		it determines an \textbf{implicit} method: we in fact have that $k_1 = h\, f(x_k + h, y_k + k_1)$ where the solution is implicit in $k_1$; considering that such relation can be regarded as $k_1 = h f(x_{k+1}, y_{k+1})$ what we determine is the implicit Euler method (equation \ref{eq:ode:impliciteuler}, page \pageref{eq:ode:impliciteuler})
		\[ y_{k+1} = y_k + h \, f\big(x_{k+1}, y_{k+1}\big) \]
		
		If we in general have that all the elements $A_{ij}$ with $j\geq j$ are all zeros, then the Runge-Kutta scheme is explicit meaning that all the coefficients $\vett k_1,\vett k_2,\dots, \vett k_s$ can be computed consecutively. \textit{Graphically} this means that the matrix $A$ must have non-zero terms only below its principal matrix not included (it means that $A_{ii}$ must always be zero).
		
		\subsubsection{One step methods}
		Usually we refer to \textbf{\textit{one step methods}} the ones that allow to explicitly compute the next step as function of the current step in the form
		\[ y_{k+1} = \phi(x_k,y_h,h) \]		
		An example is the explicit Euler method (equation \ref{eq:ode:expliciteuler}, page \pageref{eq:ode:expliciteuler}) that's characterized by the function $\phi(x_k,y_k,h) = y + h f(x_k,y_k)$. Also implicit methods can be one step; considering the implicit Euler (equation \ref{eq:ode:impliciteuler}, page \pageref{eq:ode:impliciteuler}) it can be considered as 
		\[ y_{k+1} = y_k + K_1 \hspace{2cm} \textrm{with } K_1 = h\, f\big(x_{k+1}, y_k + K_1\big) \]
		the value $K_1$ is formally a function of the parameters $x_k,y_k,h$, hence
		\[ G\big(K_1, x_k,y_k,h\big) = K - h\, f\big(x_k + h y_k + K\big) \hspace{2cm} \Rightarrow \qquad \phi(x,y,h) = y + K(x,y,h) \]
		
		In general all Runge-Kutta methods (both explicit and implicit) can be formally written in the form $y_{k+1} = \phi(x_k,y_k,h)$ and so are one step methods.
		
		\subsubsection{Error propagation}
		Known that each Runge-Kutta is a one-step method, then we can express the \textbf{local truncation error} $\tau_k(h)$ as the difference between the theoretical computed value and the numerical result obtained:
		\[ y\big(x_{k+1}\big) = \phi\big(x_k, y(x_k),h\big) + \tau_k(h) \hspace{1.4cm} \Rightarrow \hspace{1.4cm} \tau(h) = y(x+h) -\phi\big(x,y(x),h\big) \]
		Considering the \textbf{error} $\varepsilon_k = y\big(x_k\big) - y_k$ as the difference between the analytical solution and the numerical approximated one, we can regard the two cases as
		\begin{align*}
			(i): && \qquad y_{k+1} & = \phi(x_k,y_k,h) \\ 
			(ii): && \qquad y\big(x_{k+1}\big) & = \phi\big(x_k,y(x_k),h\big) + \tau_k(h)
		\end{align*}
		performing the difference $(i)-(ii)$ what we obtain is
		\[ \varepsilon_{k+1} = \Big( \phi\big(x_k,y(x_k),h\big) - \phi(x_k,y_k,h) \Big) + \tau_k(h)\]
		Considering the simple case of the explicit Euler method characterized by a function $\phi(x,y,h) = y + h\, f(x,y)$, the difference in the parenthesis is in the form $\phi(x,z,h) - \phi(x,y,h) = z - y + h\big(f(x,z) - f(x,y)\big)$; computing it's absolute value we have and considering that $f$ is lipschitzian we have
		\[\big|\phi(x,z,h)-\phi(x,y,h) \big|  \leq |z-y| + h \big| f(x,z)- f(x,y) \big| \leq (1+hL) |z-y|\]
		We can so rewrite the magnitude of the error as
		\[ |\varepsilon_{k+1}| \leq (1+hL) \big| y(x_k) - y_k \big| + |\tau_k(h)| \leq (1+hL) |\varepsilon_k| + |\tau_k(h)| \]
	
	
		\textbf{MIN 7.08}
	
	
	
	