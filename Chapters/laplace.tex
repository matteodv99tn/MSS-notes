\chapter{Laplace Transform}
	
	The \de{Laplace Transform  $\L$} is a powerful operator that allow to express a function $f(t)$ in the domain of the time $t$ to a function $\hat f(s)$ expressed in the domain of the \textbf{complex variable} $s$; in a mathematical way the passage from one domain (in this case time) to another (complex variable) is expressed as 
	\[ f(t) \mapsto \hat f(s) = \laplace{f(t)} \]
	
	Not all function $f$ can be transformed, and this is due to the existence (or not) of the following integral that is used to calculate the transform of the function:
	\begin{equation}
		\hat f(s) = \int_{0^-}^\infty f(t) e^{-st} dt = \lim_{\varepsilon\rightarrow 0^+} \lim_{M\rightarrow \infty} \int_{-\varepsilon}^M f(t) e^{-st}\, dt
	\end{equation}
	
	This mathematical tool is very powerful because it can transform \textbf{differential equation} (in the domain of the time) \textbf{into algebraic equation} (in the domain of $s$) which are much easier to solve.
	
	\begin{note}
		This concept can be seen with a logarithm analogy: the product of two number, by the logarithm rule, is easier to calculate because
		\[ a\cdot b \mapsto \log a + \log b \]
		so by this with the \textbf{logarithm} you can convert \textbf{product} into \textbf{sums} that are easier to manipulate.
	\end{note}

	In mechanical system is often required to solve linear differential equation in order to describe the time response of the system itself: this can be done with analytical techniques (such the \textit{constant variations} method), but can be very tricky to solve, or by using the Laplace transform as follows:
	\begin{itemize}
		\item with the \textbf{Laplace transform} the differential equation is converted into an algebraic one;
		\item by analyzing this equation you can determine the \textbf{frequency response} of the system object of study;
		\item with the \textbf{Laplace inverse transform} it's possible to re-convert the solution from the domain of the complex variable $s$ into the domain of time $t$. 
	\end{itemize}

\section{Transform properties}
	
	The Laplace transform has some important properties that can simplify the hand-made calculus operation; the first important thing to keep is mind is that the Laplace transform is a \de{linear operator}, so for all functions $f(t)\mapsto \hat f(s)$ and $g(t)\mapsto \hat g(s)$ and real constants $a,b\in \mathds R$ it's true that
	\begin{equation}
		h(t) := a \, f(t) + b\, g(t) \mapsto \hat h(s) := a\, \hat f(s) + b \, \hat g(s)
	\end{equation} 
	
	\begin{demonstration} \label{lap:dem:linearity}
		The linearity property can be demonstrated by applying the Laplace transform equation to the linear combination of two function:
		\begin{align*}
			\laplace{a \, f(t) + b\, g(t)} & = \int_{0^-}^\infty \Big( a\, f(t) + b\, g(t)\Big)e^{-st}\, dt \\
			& = a \lapint f(t) e^{-st}\, dt + b \lapint g(t) e^{-st}\, dt \\
			& = a \, \hat f(s) + b \, \hat g(s)
		\end{align*}
		Note that in order to prove this property we had to use the property of the integral that allowed us to split him in two separate integrals: this can be true in a general case but some function might not satisfy this option, like it can be seen in the example \ref{lap:ex:integrationproblem}.
	\end{demonstration}
	
	\begin{example}{: integrable and non-integrable function} \label{lap:ex:integrationproblem}
		Consider the piecewise defined function (dependent from the parameter $n$) for $t\geq 0$
		\[ f_n(t) = \begin{cases}
			nt & 0 \leq t \leq 1/n \\
			2-nt \qquad & 1/n \leq t \leq 2/n \\ 0 & \textrm{otherwise}
		\end{cases}\]
		By pushing the value $n$ to infinity we can see that the function $f(t)=\lim_{n\rightarrow \infty} f_n(t)$ has always a value of zero $\forall t$; by computing the integral (from zero to infinity) it's possible to calculate the area generated by this function that's equal to
		\[ F_n(t) = \int_0^\infty f_n(t)\, dt = \frac 1 n \qquad \xrightarrow{n\rightarrow \infty} \quad 0\]
		So the area associated to this function, if pushing $n\rightarrow \infty$, is zero, as expected because the function is always zero in it's domain, so this might give the (bad) idea that's possible to take out the limit of the integral outside, in the sense that
		\[  F(t) = \int_0^\infty \lim_{n\rightarrow \infty} f_n(t) \, dt = \lim_{n\rightarrow \infty} \int_0^\infty f_n(t)\, dt\]
		
		\vspace{3mm}
		Considering now another piecewise function $g_n$ defined for $t\geq 0$
		\[ g_n(t) = \begin{cases}
			n^2t & 0 \leq t \leq 1/n \\
			2n-n^2t \qquad & 1/n \leq t \leq 2/n \\ 0 & \textrm{otherwise}
		\end{cases}\]
		As in the previous case by computing the limit we see that the function $g = \lim_{n\rightarrow \infty} g_n$ is always zero in it's domain; now if we consider the position of the limit we can see that the result of the computed area differs, in fact
		\[ \lim_{n\rightarrow\infty} \int_0^\infty g_n(t)\, dt = \frac{\frac 2 n n}{2} = 1 \qquad \neq \qquad \int_0^\infty \lim_{n\rightarrow \infty} g_n(t)\, dt = \int_0^\infty 0\, dt = 0 \]
		So in a general we have to keep in mind that
		\[ \lim \int \quad \neq \quad \int\lim \]
		This concept, for example, must be kept in mind when applying the linearity rule (or in general any other properties).
	\end{example}

	
	Another important fact is associated to the \de{scale change} of the time axes; in particular by stretching/expanding the time axes by a value $a>0$ of a function $f(t) \mapsto \hat f(s)$ it's true that:
	\begin{equation}
		f(at) \mapsto \frac 1 a \, \hat f\left(\frac s a\right)
	\end{equation}

	\begin{demonstration}
		As in demonstration \ref{lap:dem:linearity}, the property of the scale change can be verified by using the definition of the Laplace transform using the change of variables $at = z$ (that means $t= z/a$):
		\begin{align*}
			\laplace{f(at)} & = \int_{0^-}^\infty f(at)\, dt \\
			& = \lapint f(z) e^{-sz/a} \, \frac{dz}{a} \\
			& = \frac 1 a \, \hat f\left(\frac s a\right)
		\end{align*}
	\end{demonstration}
		
	Other two important properties are related to the \de{translation} in respect to the $s$ axes as in respect to the $t$ axes (by a coefficient $a>0$) by using the relation that follows:
	\begin{equation}
		e^{at} f(t) \mapsto \hat f(s-a) \qquad \, \qquad f(t-a) \mapsto e^{-at} \hat f(s)
	\end{equation}

	\begin{demonstration}
		The property of the translation in respect of the $s$ complex variable is done as follows:
		\begin{align*}
			\laplace{e^{at}f(t)} & = \lapint e^{at} f(t) e^{-st} \, dt = \lapint f(t) e^{(a-s)t} \, dt \\ & = \hat f(s-a)
		\end{align*}
		The demonstration of the translation in respect to time $t$ is a little bit longer and it involves the change of coordinates $z = t-a$:
		\begin{align*}
			\laplace{f(t-a)} & = \lapint f(t-a) e^{-st} \, dt = \int_{-a}^\infty f(z) e^{-s(z-a)} \, dz \\ 
			& = \cancel{e^{-sa} \int_{-a}^{0^-} f(z) e^{-sz}\, dz} + e^{-sa} \lapint f(z) e^{-sz}\, dz 
			\\ & = e^{-as} \hat f(s)
 		\end{align*} 
 		During the decomposition it's possible to cancel out the first integral (second step) because in general we consider function $f(t)$ that are always zero for $t<0$, so by computing the integral that always going to be zero. 
	\end{demonstration}
	
\section{Existence of the transform}
	Note that not all function can be transformed using the Laplace operator $\L$; if, for example, we choose the function $f(t) = e^{t^2}$, by applying the Laplace transform to this expression we get the following integral
	\[\laplace{e^{t^2}} = \lapint e ^{t^2-st} \, dt = \int_{0^-}^T e^{(t-s)t}\, dt + \int_T^\infty e^{(t-s)t}\, dt  \]
	If we consider a costant $T > \re{s}$ we can notice that the second integral $\int_T^\infty e^{(t-s)t}\, dt$ is not convergent for any variable $s\in\mathds C$.
	
	\begin{demonstration}
		The full mathematical demonstration of this exercise can be seen as follows. Given the function $f(t) = e^{t^2}$, if it's transform exists (and we call it $\hat f(s)$) should be equal to
		\[  \hat f(s) = \lim_{M\rightarrow \infty} \int_{0^-}^M e^{t(t-s)} \, dt \]
		This is a complex integral: if we consider that $s$ is a complex variable, that means that can be expressed in the form $s = a + ib$ (where $i=\sqrt{-1}$ and $a,b\in \mathds R$). Substituting this relation in the formal expression of the transform with get the argument of the integral that is equal to $e^{t(t-a-ib)} = e^{t(t-a)} e^{-ib}$. Using the De-Moire formula\footnote{$\ e^{A+iB} = e^A e^{iB} = e^A(\cos B + i \sin B)$. } it's possible to re-write the previous integral as		
		\[  \lim_{M\rightarrow \infty} \int_{0^-}^M e^{t(t-a)} \Big( \cos (tb) - i \sin(tb)  \Big) \, dt \]
		\[ \Rightarrow \qquad \textrm{Re}: \ \lim_{M\rightarrow \infty} \int_{0^-}^M e^{t(t-a)} \cos (tb) \, dt \qquad 
		\textrm{Im}: \ \lim_{M\rightarrow \infty} \int_{0^-}^M e^{t(t-a)} \sin (tb) \, dt \]
		Considering the real part of the integral we can see that it's made of a cosine function that multiplies an exponential (that's monotonically ascendant function). Considering now a point $M$ on the time axes where the cosine start increasing from zero and a consequent point with relative distance $\Delta M$ we get that the integral up to $M$ is less then the area up to $M+\Delta M$ and so with that we can see some information:
		\begin{align*}
			\int_0^{M+\Delta M} e^{t(t-a)} \cos(tb)\, dt & \geq \int_0^M e^{t(t-a)} \cos(tb)\, dt + \int_M^{M+\Delta M} e^{M(M-a)} \cos(tb)\, dt \\
			& \geq \int_0^M e^{t(t-a)} \cos(tb)\, dt + e^{M(M-a)} \int_M^{M+\Delta M} \cos(tb)\, dt 
		\end{align*}
		this inequality is true because the exponential keeps growing and so by considering it constant in the range $[M,M+\Delta M]$ will give an integral with less value. This integral has a residual part that's not going to zero, and thus the limit doesn't converge. We have in fact to keep in mind that an improper integral exists if and only if the next equation is verified:
		\begin{equation}
			\lim_{a,b \rightarrow \infty} \int_a^b f(t) \, dt = 0
		\end{equation}
	\end{demonstration}

	\paragraph{Exponential order functions} A function $f(t)$ can be seen as of \textbf{exponential order} function if only happens that $|f(t)| \leq M e^{Nt}$ after a certain time $t \geq T$ (and $M,N\in \mathds R$ are two constants); in this case (if $f(t)$ is a exponential order function) it's true that \de{Laplace transform always exists} (in at least a part of the complex plane).
	
	\begin{demonstration}
		To demonstrate that a exponential order function has always a Laplace transform by considering only the residual part of the integral, so ranging not from $0^-$, but from $T$ to infinity. In particular we can verify the all the following inequalities are correct:
		\begin{align*}
			\left| \int_0^T f(t) e^{-st} \, dt \right| & \leq \left| f(t) e^{-st} \right| \, dt 
			 \leq \int_0^\infty |f(t)| \left|e^{-st}\right| \, dt \\ 
			& \leq \int_0^\infty M e^{Nt} \left|e^{-st}\right| \, dt
		\end{align*}
		By using the De-Moire formula it's possible to rewrite the $|e^{-st}|$ term that end up resulting
		\begin{align*}
			\left|e^{-st}\right| & = \left| e^{-at} \big(\cos(bt) + i \sin(bt)\big) \right| = e^{-at} \big|\cos(bt) + i\sin(bt)\big| \\
			& = e^{-at} \cancel{\sqrt{\cos^2(bt)  + \sin^2(bt)}}
		\end{align*}
		At this point we can continue in the analysis of the inequalities keeping in mind that now on $a = \textrm{Re}(s)$:
		\begin{align*}
			\left| \int_0^T f(t) e^{-st} \, dt \right| & \leq \int_T^\infty Me^{Nt} e^{-at} \, dt = M \int_T^\infty e^{(N-a)t} \, dt
		\end{align*}
		
		At this point if $a>N$ we see that the residual $e^{(N-a)t} \xrightarrow{t\rightarrow \infty}0$ goes to zero, hence the integral should have a limited value.	
	\end{demonstration}
	By following the definition it's also possible to observe that the \textbf{domain} of the variable $s$ for the Laplace transform of an exponential order function is \textbf{at least} $\forall s = a + ib \in \mathds C$ such that $a = \textrm{Re}(s) \geq N$.

\section{Usage of the Laplace transform}
	Imagine the problem of solving the following differential equation problem
	\[ \begin{cases}
		y'(t) = ay(t) + t \\ y(0) = 1 
	\end{cases} \]
	Solving this equation in the domain of time can be difficult in general, and that's why we introduced the Laplace transform and the analysis in the domain of the complex variable $s$. By computing the Laplace transform on the differential equation we get the expression
	\begin{align*}
		\laplace{y'(t)}(s) & = \laplace{a y(t) + t} (s) \\ & = a \laplace{y(t)} (s) + \laplace{t}(s)
	\end{align*}
	At this point we have to describe an important \textbf{Laplace transform property} that states that the transform of the first derivative $f'(t)$ of a function $f(t) \mapsto \hat f(s)$ is equal to the transform $\hat f(s)$ multiplied by the complex variable $s$:
	\begin{equation}
		f'(t) \mapsto s\, \hat f(s) - f(0^+)
	\end{equation} 
	where $f(0^+)$ is the value of the function $f(t)$ at the origin of the time axes and, de-facto, represents the initial (Cauchy) condition of the ordinary differential equation problem.
	
	This property allow us to solve \de{differential equation} in the time domain as \de{algebraic equations} in the complex variable world; returning back to the initial problem we can rewrite the differential equation as
	\[ s \, \hat y(s) - \underbrace{1}_{y(0^+)} = a \, \hat y(s) + \underbrace{\frac 1 {s^2}}_{\laplace{t}} \qquad \Rightarrow \quad \hat y(s) = \frac{1 + \frac 1 {s^2}}{s-a} \]
	\begin{note}
		To do the calculation it's useful to refer to the property table (\ref{app:lap:properties}) and the common functions transforms (table \ref{app:lap:transforms}) on page \pageref{app:lap:transforms}.
	\end{note}	
	
	\begin{example}{: system of ordinary differential equation}
		Consider the following system of 2 ordinary differential equation subjected to their initial condition as follows:
		\[ \begin{cases}
			x'(t) = y(t) + 1 \\ y'(t) = x(t) + \cos t \\ x(0) = 1 \\ y(0) = 1
		\end{cases} \]
		To solve this particular problem we can apply the rules of the Laplace transform to determine the system of 2 equation in the domain of the complex variable $s$ as follows:
		\begin{align*}
			i)&&  \underbrace{s \tilde x(s) - x(0)}_{\laplace{x'(t)}}&=\tilde y(s) + \frac 1 s \\
			ii)&&  \underbrace{s \tilde y(s) - y(0)}_{\laplace{y'(t)}}&=\tilde x(s) + \frac 1 {1+s^2}
		\end{align*}
		Knowing the initial condition of the system it's possible to explicit the system of the function $\tilde x,\tilde y$ by using a matrix notation arriving to the following result:
		\[ \underbrace{\begin{bmatrix}
			s & -1 \\ -1 & s
		\end{bmatrix}}_{A(s)} \begin{pmatrix}
			\tilde x(s) \\ \tilde y(s)
		\end{pmatrix} = \begin{pmatrix}
			1 + \frac 1 s \\ \frac  1 {1+s^2}
		\end{pmatrix} \]
		The explicit value of the variables $\tilde x,\tilde y$ can be calculated by inverting the matrix $A(s)$ arriving to the following results:
		\[ \tilde x (s) = \frac{s+1 - \frac 1 {1+s}}{s^2-1} \qquad \tilde y(s) = \frac{\frac s {1+s} - 1  - \frac 1 s }{s^2-1} \]
		
	\end{example}

\section{Inversion of the Laplace transform: partial fraction expansion}
	
	The \de{inversion} of a Laplace transform is the step that allow to \textit{transport} the algebraic solution found in the $s$ domain into a time description, and so it's represented by the operator $\aL$.
	
	An important thing to notice is that while dealing with (systems of) ordinary differential equation is that the result of the transform (such as $\hat x(s)$) is usually expressed as a rational polynomial in the form $P(s)/Q(s)$ (where $P,Q$ are so two polynomial with real coefficients in the variable $s$). It's important also to note that, very often in real application, the degree $\partial P$ of the numerator is less than the degree of the denominator $\partial Q$. When this doesn't \textit{naturally} happen, the step to follow is to use the polynomial division: considering in fact that $P(s)$ in this case can always be rewritten as $T(s)Q(s) + R(s)$, it's easy to see that
	\[ \frac{P(s)}{Q(s)} = \frac{T(s)Q(s) + R(s)}{Q(s)} = T(s) + \frac{R(s)}{Q(s)} \]
	The part represented by the polynomial $T(s)$ that has real coefficients is associated to the transform of Dirac pulses function $\delta(t)$: we can in fact note
	\begin{equation}
		\begin{split}
			\laplace{\delta(t)}(s) & = \lapint \delta(t) e^{-st}\, dt = e^{-st} \Big|_{t=0} \\ & = 1 \\
			\laplace{\delta'(t)}(s) & = -\lapint \delta'(t) e^{-st} \, dt = - \frac d {dt} \left(e^{-st}\right) \Big|_{t=0} = s e^ {-st} \Big|_{t=0} \\ & = s
		\end{split}
	\end{equation} \vspace{0.5cm}

	Assumed that we now have a transform solution that's a simplified rational polynomial $P(s)/Q(s)$ such that $\partial P < \partial Q$, we can try to compute it's inverse, so the same function but expressed in the time domain. In practise this is done by using the \de{partial fraction expansion} of the rational polynomial: this allows to re-state the solution as a combination of simpler \textit{elements} that can be easily inverted by simply looking a the Laplace table.
	
	In order to properly do a partial fraction expansion is important to re-write the denominator of the polynomial ratio in a factorised form such as
	\[ \frac{P(s)}{Q(s)} = \frac{b_0 + b_1 s + b_2s^2+ \dots + b_ms^m}{\big( s-p_1 \big)^{m_1} \big( s-p_1 \big)^{m_2} \dots \big( s-p_1 \big)^{m_n} } \]
	where $p_i$ are the roots of the denominator everyone having a molteplicity $m_i$. Depending on the molteplicity of the root and their kind (real or conjugated complex) the procedure to accomplish a partial fraction example can be different. In the following paragraph each possibility will be presented with an example.
	
\subsection{Real roots}
	\paragraph{Single real root} Let's consider the following rational polynomial with no multiple roots:
	\[ G(s) = \frac{s}{(s-1)(s+3)(s-4)} \]
	It's factorization is based on determining the coefficients $\alpha_i$ such that
	\[ i): \qquad \frac{\alpha_1}{s-1} + \frac{\alpha_2}{s+3} + \frac{\alpha_3}{s-4} = \frac{s}{(s-1)(s+3)(s-4)} \]
	This can be accomplished in two way: the first one is to compute the some of the 3 \textit{basic} element in a parametric form depending on $\alpha_i$ and then solving the linear system in order to determine the correct parameters to satisfy the equality. A second (smarter) way is instead considering that each coefficient can be determined as
	\[ \alpha_i = \lim_{s\rightarrow p_i} \big(s-p_i\big) G(s) \]
	Considering the example, to compute the coefficient $\alpha_1$ associated to the root $p = 1$ we can multiply the relation $i)$ with a factor $(s-1)$: this gives us the equation
	\[ \frac{s}{\cancel{(s-1)}(s+3)(s-4)} \cancel{(s-1)}= \frac{\alpha_1}{\cancel{s-1}} \cancel{(s-1)} + {(s-1) \left( \frac{\alpha_2}{s+3} + \frac{\alpha_3}{s-4} \right)}  \]
	Evaluating this expression at the point $s=1$ will result in an equation whose only unknown coefficient is $\alpha_1$ (because $\alpha_2,\alpha_3$ are \textit{eliminated} to the multiplication with $s-1$ that goes to zero), and so
	\[\xrightarrow{s=1} \quad \alpha_1 = \frac{1}{(1+3)(1-4)} = -\frac 1 {12} \]
	Repeating this process also for the other two roots ($-3,4$) we retrieve the coefficients $\alpha_2 = -\frac 3 {28}$ and $\alpha_3 = \frac 4 {21}$: with that is possible to verify that
	\[ -\frac 1 {12} \frac 1 {s-1} - \frac 3 {28} \frac 1 {s+3} + \frac 4 {21} \frac{1}{s-4} = \frac{s}{(s-1)(s+3)(s-4)} \]
	Each basic element can be easily transform using the Laplace table, and in particular
	\[ \antilaplace{G(s)}(t) \ : \quad -\frac 1 {12} e^{-t} - \frac 3{28} e^{3t} + \frac 4 {21} e^{-4t} \]
	
	\paragraph{Multiple real roots} Let's consider now a case in which the rational polynomial has multiple roots such as
	\[ G(s) = \frac{1+s}{(s-4)^4} \]
	In this case the partial fraction has to be made not with 4 equals element with denominator $s-4$, but instead consider four increasing exponential terms as follows:
	\[ i) \qquad \frac{\alpha_1}{s-4} + \frac{\alpha_2}{(s-4)^2} + \frac{\alpha_3}{(s-4)^3} + \frac{\alpha_4}{(s-4)^4} = \frac{1+s}{(s-4)^4} \]
	This problem can be solved by summing up all the elements and solving the linear system in respect to $\alpha_i$, but a smarter way is to consider that the element $a_{n-k}$ (where $n$ is the number of the multiple root, in this case $4$) can be expressed as $\frac 1 {k!} \lim_{s\rightarrow p} \frac{d^k}{ds^k} \big[ (s-p)^n G(s) \big]$; this process is easier to see considering the stated example. The first thing is to multiply $i)$ with the polynomial $(s-4)^4$ arriving to the relation
	\[ 1+s = \alpha_1(s-4)^3 + \alpha_2(s-4)^2 + \alpha_3(s-4) + \alpha_4 \]
	Evaluating this expression for $s=4$ determines the coefficient $\alpha_4 = 5$; in order to determine the other coefficient is to use the proposed \textit{scheme} where each time we differentiate (in respect to $s$) the previous equation and evaluate at the point $s=4$:
	\begin{align*}
		& \downarrow d/ds \\ 
		1 & = 3 \alpha_1 (s-4)^2 + 2 \alpha_2(s-4) + \alpha_3 \qquad & \xrightarrow{s=4} \quad \alpha_3 = 1 \\
		& \downarrow d/ds \\ 
		0 & = 6 \alpha_1 (s-4) + 2 \alpha_2 \qquad & \xrightarrow{s=4} \quad \alpha_2 = 0 \\
		& \downarrow d/ds \\ 
		0 & = 6 \alpha_1 (s-4) \qquad & \xrightarrow{s=4} \quad \alpha_1 = 0 \\
	\end{align*}
	Having determined the coefficients $\alpha_i$ associated to the partial fraction expansion it's possible to compute the inverse transform of the original $G(s)$ that's
	\[ \antilaplace{G(s)} = \antilaplace{\frac 1 {(s-4)^3}} +5 \antilaplace{\frac 1 {(s-4)^4 }} = \frac 1 2e^{4t}t^2 + 5 \frac 1 6 e^{4t}t^3   \]
	
	\begin{example}{: partial fraction expansion with different roots, one which is multiple}
		Let's consider the following rational polynomial $G(s)$ that's already been split in the factor for the partial expansion:
		\[G(s):= \frac{1+s}{(s-1)(s+2)^3} = \frac{A}{s-1} + \frac{B}{s+2} + \frac{C}{(s+2)^2} + \frac{D}{(s+2)^3} \]
		In order to calculate the first coefficient $A$ related to the root $s=1$ we simply need to multiply the last equation with a factor $(s-1)(s+2)^2$ and then evaluate the result in respect to $s=1$:
		\begin{align*}
			1+s & = A(s+2)^3 + (s+1) \Big[ B(s+2)^2 + C(s+2) + D \Big] \\
			\xrightarrow{s=1} \qquad 2 & =A 3^3 \qquad \Rightarrow \quad A = \frac 2 {27}
		\end{align*}
		The next step is to calculate the remaining coefficients ($B,C,D$) associated to the multiple root $s=-2$, however with this equation as now stated we cannot apply the method shown; in order to create a proper equation we need to \textbf{deflate} it, so by expanding the $(s+2)^3$ associated to the $A$ coefficient and moving it on the left hand side:
		\begin{align*}
			1+s - \frac 2 {27} \Big( s^2 + 6s^2 + 12s +8 \Big) & = (s+1) \Big[ B(s+2)^2 + C(s+2) + D \Big] \\
			\frac 1 {27} \Big( -2s^3 -12s^2+35s +11 \Big) & =
		\end{align*}
		At this point we can divide both sides of the equation by a factor $s-1$ (if this cannot be done on the left hand side, this means that an error in calculation has been done), and so by performing the polynomial division we get the equation
		\[ \frac 1{27} \left( -2s^2 -14s -11 \right) = B(s+2)^2 + C(s+2) + D \]
		From this point onward it's possible to use the method shown to calculate the coefficient of the multiple roots; starting evaluating the previous expression at point $s=-2$ we get the value $D = \frac{-8+28-11}{27} = \frac 1 3$. Continuing with differentiation we get
		\begin{align*}
			& \downarrow d/ds \\ 
			\frac{-4s -14}{27} & = 2B(s+2) + C  \qquad & \xrightarrow{s=-2} \quad C = -\frac 2 9 \\
			& \downarrow d/ds \\ 
			\frac{-4}{27} & = 2B  & \xrightarrow{s=-2} \quad B = -\frac 2 {27} \\
		\end{align*}
	
		To end the inversion of the starting transform $G(s)$ we can see that
		\begin{align*}
			\antilaplace{G(s)}(t) & = \frac 2 {27} \laplace{\frac 1 {s-1}} - \frac 2 {27} \laplace{\frac 1 {s+2}} - \frac 2 {9} \laplace{\frac 1 {(s+2)^2}}  + \frac 1 3 \laplace{\frac 1 {(s+2)^3}} \\
			& = \frac 2 {27} e^t - \frac 2 {27}e^{-2t} - \frac29 e^{-2t}t + \frac 1 3 \cdot \frac 1 2 e^{-2t} t^2
		\end{align*} 		
	\end{example}
	
\subsection{Complex roots} 
	Dealing with complex roots on the denominator $Q(s)$ of the solution of the ordinary differential equation has an higher computational difficulty in respect to the real one. An observation that can be done is that if the complex value $\alpha = a + ib \in \mathds C$ is a root of the denominator, so that $Q(\alpha) = 0$, than also it's conjugate $\alpha^* = a - ib$ it's a root. Considering in fact that the polynomial denominator $Q(s)$, evaluated in the point $s = \alpha$, can be expressed as
	\[Q(\alpha) = q_0 + \alpha q_1 + \alpha^2 q_2 + \dots + \alpha^n q_n = 0 \]
	where $q_i$ are real coefficients. At this point we have to consider the following property of the conjugate that are true for all couple of complex variables $\alpha,\beta$:
	\begin{equation}
	\begin{split}
		i) & \qquad \big( \alpha + \beta \big)^* = \alpha^* + \beta^* \\
		ii) & \qquad \big(\alpha \beta\big)^* = \alpha^*\beta^* \\
		iii) & \qquad \alpha \alpha^* = |\alpha|^2
	\end{split}
	\end{equation}

	Now known that $Q(\alpha) = 0$, than also the conjugate $Q^*(\alpha)$ should be equal to zero (because $0^* = 0$) and so by applying the properties we can see that
	\begin{align*}
		0 & = Q(\alpha) = Q^*(\alpha) = \Big(q_0 + \alpha q_1 + \alpha^2 q_2 + \dots + \alpha^n q_n \Big)^* \\
		& = q_0^* + \alpha^* q_1^* + \alpha^{*2} q_2^* + \dots + \alpha^{*n} q_n^* \\
		& = q_0 + \alpha^* q_1 + \alpha^{*2} q_2 + \dots + \alpha^{*n} q_n
		= Q\big(\alpha^*\big)
	\end{align*}
	This verifies that if $\alpha$ is a root of $Q(s)$, than also it's conjugate $\alpha^*$ is a root of the same polynomial and so we can rewrite the denominator in the form
	\[ Q(s) = \dots \underbrace{\big(s-\alpha\big) \big(s-\alpha^*\big)}_\textrm{conj. roots} \dots \]
	
	\paragraph{Single conjugated complex roots} Let's now assume that we have a rational polynomial that present two conjugated complex roots expressed in the form
		\[ \frac{P(s)}{Q(s)} = \frac{P(s)}{(s-\alpha)(s-\alpha^*)} = \frac A {s-\alpha} + \frac{B}{s-\alpha^*} \]
	By trying to apply the partial fraction expansion as for the real root previously described, we can demonstrate that the two coefficients $A$ and $B$ are conjugated, so such that $B = A^*$. In fact by summing the two elements we can see that
	\begin{align*}
		\frac A {s-\alpha} + \frac{B}{s-\alpha^*} & = \frac{A(s-\alpha\c) + B(s-\alpha)}{(s-\alpha)(s-\alpha\c)} = \frac{(A+B)s - (A \alpha\c+ B\alpha)}{(s-\alpha)(s-\alpha\c)} 	
	\end{align*}
	By expanding the definition of the complex variable (so expanding $A = a+ib$, $B = c+id$...) and considering that the polynomial $P(s)$ at the numerator has only real coefficients it's possible to see that, in order to verify the equality, it must be $a = c$ and $b = - d$, and so the two complex values $A$ and $B$ must be conjugated.
	
	\vspace{3mm} Proven that $B = A\c$ now the problem is to find such complex value $A$ in order to satisfy the partial fraction expansion; in order to do so we can try to use the same approach shown for real roots by multiplying both members of the equation by $Q(s)$ and then evaluating the result for $s = \alpha$:
	\[ \frac{P(s)}{(s-\alpha)(s-\alpha^*)} = \frac A {s-\alpha} + \frac{B}{s-\alpha^*} \qquad \xrightarrow[Q(s)]{\times} \qquad P(s) = A(s-\alpha\c)	+ A\c(s-\alpha)  \]
	\[ \Rightarrow \qquad P(\alpha) = A (\alpha-\alpha\c) + \cancel{A\c (\alpha-\alpha)}\]
	Considering now that $\alpha-\alpha\c$ conresponds to a purely imaginary number equals 2 times the imaginary part of the root $\alpha$, we can invert that relation in order to explicitly get $A$:
	\[ A =\frac{P(\alpha)}{2 i \im{\alpha}} = - i \frac{P(\alpha)}{2\im \alpha} \]
	
	\vspace{3mm}
	Let's now try to consider a real case scenario where we want to deal with the inversion of the following rational polynomial (that's consequently expanded):
	\[ \frac s { \big( s - (1+i) \big)\big( s - (1-i) \big) } = \frac A {s - (1+i)} + \frac {A\c}{s-(1-i)}  \]
	We can now multiply both members of the equation by the denominator $Q(s)$ and then evaluate the result for the point $s  = 1+i$ (root of the denominator):
	\[ s = A \big(s-(1-i)\big) + A\c\big(s-(1+i)\big) \qquad \xrightarrow{s = 1+i} \quad 1+i = A \big(1+i - 1 + i\big)\]
	\[ \Rightarrow \qquad A = \frac{1+i}{2i} = \frac 1 2 \big(1 - i	\big) \]
	With this value retrieved we can now state that
	\[ \frac s { \big( s - (1+i) \big)\big( s - (1-i) \big) } = \frac 1 2 \left(\frac {1-i} {s - (1+i)} + \frac {1+i}{s-(1-i)} \right)  \]
	Unfortunately watching the Laplace table we can see there's no possible match between the known transform (where the numerator is expressed as a pure real number) and the complex coefficient retrieved. But an interesting fact that we can note is that by \textit{merging} the two components we can get a rational polynomial with real coefficient: in a general way we can in fact state that
	\begin{align*}
		\frac A {s-\alpha} + \frac{A\c}{s-\alpha\c} & = \frac{A(s-\alpha\c) + A\c(s-\alpha)}{(s-\alpha)(s-\alpha\c)} \\
		& = \frac{ s\big(A+A\c\big) - A\alpha\c - A\c\alpha }{s^2 - s \big(\alpha+\alpha\c\big) + |\alpha|^2}
	\end{align*}
	This rational polynomial can be matched with the following transform:
	\begin{align*}
		\laplace{Ce^{at}\cos(\omega t) + D e^{at}\sin(\Omega t)} & =C \frac{s-a}{(s-a)^2+\omega^2} + D \frac{\omega}{(s-a)^2 + \omega^2} \\
		& = \frac{Cs - \big(D\omega - Ca\big)}{(s-a)^2 + \omega^2 }
	\end{align*}
	Given now the values of $\alpha,A$, the problem is to find the expression of the coefficients $a,\omega,C$ and $D$; in order to to so we can perform two steps:
	\begin{enumerate}
		\item match the denominator in order to find $a$ and $\omega$; in order to compare the denominator it's useful to expand the denominator $(s-a)^2 + \omega^2 = s^2  - 2sa + a^2 + \omega^2$. At this point by relating the function we can see that
		\[ \cancel {s^2} - s \big(\alpha+\alpha\c\big) + |\alpha|^2 = \cancel{s^2}  - 2sa + a^2 + \omega^2  \]
		\[ \Rightarrow \quad \begin{cases}
			-2sa = - s\big(\alpha + \alpha \c\big) \\ \alpha^2+\omega^2 = |\alpha|^2 
		\end{cases} \qquad \Rightarrow \quad a = \frac{\alpha + \alpha\c}{2} = \re \alpha 	\]
		\[ \Rightarrow \quad \omega = \sqrt{|\alpha|^2-a^2}  = \sqrt{|\alpha|^2-\textrm{Re}^2\alpha} = \sqrt{\textrm{Im}^2\alpha} = \pm \im\alpha  \]
		
		\item at this point, known $a$ and $\omega$, it's possible to match the numerator in order to get the coefficients $C$ and $D$:
		\[ s\big(A+A\c\big) - A\alpha\c - A\c\alpha = Cs - \big(D\omega - Ca\big) \]
		\[ \Rightarrow \quad \begin{cases}
			A + A\c = C \\ -A\alpha\c - A\c\alpha = D\omega - Ca
		\end{cases} \qquad \Rightarrow \quad C = A+A\c = 2 \re{\alpha}\]
		\[ \Rightarrow \qquad D = \frac{Ca - A \alpha\c  - A\c \alpha}{\omega}  \]
	\end{enumerate}
	
	\begin{example}{: inversion of a transform with two conjugated complex roots}
		Let's consider the following rational polynomial that has to be inverted:
		\[ \frac{s+1}{s^2+s+10} \]
		At this point we can verify that the roots of the denominator are complex conjugated by simply applying the quadratic formula to retrieve them $s_{1,2} = \frac{-1\pm\sqrt{-9}}{2}$. At this point the problem of the inversion if finding the parameters $a,\omega,C,D$ such that
		\[ \frac{s+1}{s^2+s+10} = C \frac{s-a}{(s-a)^2+\omega^2} + D \frac{\omega}{(s-a)^2 + \omega^2} \]
		By comparing the denominators we can observe that $-2sa = s$ and so $a = -\frac 1 2$ and so 
		\[ a^2+ \omega^2 = 10 \qquad \Rightarrow \quad \omega = \sqrt{10-a^2} = \frac{\sqrt{39}}{2}\]
		At this point it's possible to compare the denominator (considering that now $a,\omega$ are known) that's lead to
		\[ s + 1 = C\left( s - a\right) + D\omega \qquad \Rightarrow \quad C = 1 \qquad,\quad D = \frac{1+Ca}{\omega} = \frac 1 {\sqrt{39}} \]
		
		Retrieved all the coefficient we can explicitly state the inversion of the original rational polynomial:
		\[ \laplace{\frac{s+1}{s^2+s+10} }(t) = e^{-\frac 1 2t} \left[ \cos\left( \frac{\sqrt{39}}{2} t \right) + \frac{1}{\sqrt{39}} \sin\left( \frac{\sqrt{39}}{2} t \right) \right] \]
	\end{example}
	
	\paragraph{Multiple complex roots} Dealing with multiple complex roots requires even more computational steps; let's consider the generic case of a multiple complex root of multiplicity of 2 that's already expanded:
	\[ \frac {P(s)}{(s-\alpha)^2(s-\alpha\c)^2} = \frac{A}{s-\alpha} + \frac{B}{(s-\alpha)^2} + \frac{A\c}{s-\alpha\c} + \frac{B\c}{(s-\alpha\c)^2} \]
	As in all the previously cases in order to compute the first parameter $B$ it's possible to multiply both sides of the equation by the denominator $Q(s) = (s-\alpha)^2(s-\alpha\c)^2$ and then evaluate the relation at the root $s = \alpha$:
	\[ P(s) = A (s-\alpha)(s-\alpha\c)^2 + B (s-\alpha\c)^2 + A\c(s-\alpha\c)(s-\alpha)^2 + B\c (s-\alpha)^2  \]
	\[ \Rightarrow \quad P(\alpha) = B \underbrace{\big(\alpha-\alpha\c\big)}_{=2\im \alpha} \,^2 \qquad \Rightarrow \quad B = \frac{P(\alpha)}{4 \textrm{Im}^2(\alpha)} \]
	Now in order to calculate the second parameter $A$ of the relation we have to deflate te elements associated to $B$ and then, as for the multiple real roots, compute the derivative in respect to the variable $s$:
	\begin{align*} 	
	\xrightarrow{\textrm{deflation}} \quad P(s) - B(s-\alpha\c)^2 - B\c(s-\alpha)^2  & = A(s-\alpha)(s-\alpha\c)^2 + A\c(s-\alpha\c)(s-\alpha)^2 \\
	& \, \downarrow \ d/ds \\
	P'(s) - 2B\big(s-\alpha\c\big) - 2B\c\big(s-\alpha\big) & = A \big(s-\alpha\c\big) \Big[ s-\alpha\c + s \big(s-\alpha\big) \Big] \\ & \  + A\c \big(s-\alpha\big)\Big[ s-\alpha + 2\big(s-\alpha\c\big) \Big]
	\end{align*} 

	By now evaluating the expression at the point $s = \alpha$ it's possible to have a clear definition of the complex variable $A$:
	\[ P'(\alpha) - 2B \big(s-\alpha\c\big) = A \big(\alpha-\alpha\c\big)^2 \qquad \Rightarrow \quad A = \frac{P'(\alpha) - 2B\big(s-\alpha\c\big)}{4\textrm{Im}^2(\alpha)} \]	
	
	\begin{example}{: inversion of a transform with multiple complex roots}
		Let's consider the following rational transform that has to be inverted:
		\[ \frac{s^2}{\big(s^2 + s + 1\big)^2} \]
		The first thing to notice that the multiple roots are complex because calculating their value by using the quadratic formula it happens that
		\[ s_{1,2} = \frac{-1 \pm \sqrt{1-4}}{2} = -\frac 1 2 \pm i \frac{\sqrt 3}{2} \]
		For sake of semplicity of representation of the equation, from now on these roots value will be reported as $z = -\frac 1 2 + \frac{\sqrt 3}{2}$ and $z\c = -\frac 12 - \frac{\sqrt 3}{2}$. At this point it's possible to express  the parametric partial fraction expansion of the transform as
		\[ \frac{s^2}{\big(s^2 + s + 1\big)^2} = \frac{A}{s-z} + \frac{A\c}{s-z\c} + \frac{B}{(s-z)^2} + \frac{B\c}{(s-z\c)^2}\]
		In order to retrieve the value of the coefficient $B$ we multiply both the sides by the factor $Q(s) = (s^2+s+1)^2 = (s-z)^2(s-z\c)^2$ that, evaluated at the point $s =z $, will give the expression
		\[ z^2 = B \underbrace{\big(z-z\c\big)}_{i\sqrt 3} \, ^2 \qquad \Rightarrow \quad B = -\frac{z^2}{3} = - \frac{\left(-\frac 1 2  + i\frac{\sqrt 3}{2}\right)^2}{3} = - \frac 1 6  - i \frac{\sqrt 3}{6} \]
		
		By deflating the term presenting the parameter $B$ just calculate we can get the relation
		\[ s^2 - B(s-z\c)^2 - B\c(s-z)^2 = A(s-z)(s-z\c)^2 + A\c (s-z\c)(s-z)^2 \]
		\[ \xrightarrow{d/ds} \quad 2s -2B(s-z\c) - 2B\c(s-z) = A(s-z\c)^2 +2(A + A\c)(s-z)(s-z\c) + A\c (s-z)^2  \]
		Having computed the derivative of the relation in respect to the variable $s$ it's possible to compute the parameter $A$ just by evaluating the result in the point $s = z$:
		\[ 2z - 2B\big(z-z\c\big) = A \big(z-z\c\big)^2 \qquad \Rightarrow \quad A = -\frac{2z - 4iB \im z}{4\textrm{Im}^2(z)} = - i\frac{2}{3\sqrt 3} \]
	\end{example}
	
	\paragraph{Parametric solution for multiple complex roots} To solve the partial fraction expansion of multiple complex roots it's possible to use a parametric formula where the numerator are defined as polynomials $A,B$ in a generic variable $t$; in particular for the double complex root the expansion can be expressed as
	\begin{equation}
		\frac{P(s)}{\big(s^2-as +t\big)^2 Q(s)} = \frac{A'(t)s + B'(t)}{s^2-as+t} - \frac{A(t)s + B(t)}{\big(s^2-as+t\big)^2} + \frac{q_t(s,t)}{Q(s)} 
	\end{equation}
	where the term $q_t(s,t)$ represent the derivative $\partial q/\partial t$ of the remaining term on the numerator after the partial fraction expansion. If the polynomial has 3 complex roots than the parametric decomposition becomes
	\begin{equation} \label{eq:laplace:3mulrcomplroots}
		2\frac{P(s)}{\big(s^2-2as +t\big)^3 Q(s)} = 2\frac{A(t)s + B(t)}{\big(s^2-2as+t\big)^3} - 2\frac{A'(t)s + B'(t)}{\big(s^2-2as+t\big)^2} +  \frac{A''(t)s + B''(t)}{s^2-as+t} + \frac{q_{tt}(s,t)}{Q(s)} 
	\end{equation}
	where $q_{tt}=\partial^2q/\partial t^2$.  \vspace{3mm}
	
	This process can be better seen by considering a numerical example like the one that follows:
	\[ \frac{s^2+1}{(s-1)(s^2-2s+2)^3} = \frac{A}{s-1} + \frac{B_1s + B_2}{(s^2-2s+2)^3} + \frac {C_1s + C_2}{(s^2-2s+2)^2} + \frac{D_1s+D_2}{s^2-2s+2}\]
	The first coefficient $A$ of the partial fraction expansion (associated to a single real root) can be determined easily with the methods previously described and so
	\[ A= \left. \frac{s^2+1}{(s^2-2s+2)^3} \right|_{s=1} = 2	\]
	
	At this point it's important to acknowledge hot compute the parametric functions $A,B,q$; in particular this can be done by writing the following relation:
	\[ \frac{s^2+1}{(s-1)(s^2-2s+2)} = \frac{A(t)s+B(t)}{s^2-2s+t} + \frac{q(t)}{s-1} \]
	We can note that the left hand side is equal to the original rational polynomial with the only difference that the term $s^2-2s+2$ is now considered once; the right hand side is instead the \textit{correct} partial fraction expansion of the other side with the difference that the coefficient $A,B,q$ are now function of the variable $t$ and the last known coefficient of the complex root (in this case the $+$) has been replaced with the variable $t$. \\
	By multiplying both member of the equation by the denominator of the left hand side we derive the following equation:
	\[ s^2+1= \Big(A(t)s + B(t)\Big) (s-1) + q(t) \big(s^2-2s+t\big) \]
	This expression allows now us to compute the parametric form of the function $A,B,q$; in particular considering that $s^2-2s+t= 0$ and so $s^2 = 2s-t$ we can rewrite the previous expression as
	\[ 2s-t+1 = A(t)(2s-t) - A(t) s + B(t)s -B(t) + \cancel{q(t) \big(2s-t - 2s +t\big)} \]
	\[\Rightarrow \quad \begin{cases}
		2 & = A(t) + B(t) \\
		-t+1 & =-tA(t) - B(t) 
	\end{cases} \qquad \Rightarrow \quad A(t) = 1 - \frac{2}{t-1} \ , \quad B(t) = 1 + \frac{2}{t-1} \]
	In order to have the complete the partial fraction expansion we need to compute the derivative (up to the second order) and evaluate them for $t=3$ (noting that $A(2) = -1 $ and $B(2) = 3$):
	\[ A' = \left.\frac{2}{(t-1)^2}\right|_{t=2} = 2 \qquad B' =  \left.-\frac{2}{(t-1)^2}\right|_{t=2} = 2 \]
	\[ A'' = \left. -\frac 4{(t-1)^3} \right|_{t=2} = -4 \qquad B'' = \left. \frac 4{(t-1)^3} \right|_{t=2} = 4 \]
	
	With all the coefficient defined is possible to express numerically the partial fraction expansion by using the equation \ref{eq:laplace:3mulrcomplroots}:
	\[ \frac 2 {s-1} + \frac{-s + 3}{(s^2-2s+2)^3} -2 \frac{s-1}{(s^2-2s+2)^2} + \frac 12 \frac{-4s+4}{s^2-2s+2}\]

	

\section{Boundary value problem}
	Usually some differential equation problems requires some condition that are not defined at the initial time $t=0$ of the time axes, but in a more general point $t=t^*$: in order to solve this problem a parametric approach is recomended. As example let's consider the following problem:
	
	\[ \begin{cases}
		x''(t) + y'(t) = \sin(t) \\ y'(t) + x'(t) = 1 + \sin(t) \\
		x(0) = 0 , \quad y(0) = -1,\quad x(1) = 1
	\end{cases} \]
	
	As we can see the last condition constrains the solution $x(t),y(t)$ at the time $t=1$; in order to solve the problem in the Laplace domain we apply the transform on the system of differential equation and so
	\[ \begin{cases}
		s^2 \hat x(s) - x'(0) - s x(0)  + s\hat y(s) - y(0) = \frac 1 {s^2+1} \\
		s \hat y(s) - y(0) + s\hat x(s) - x(0) = \frac 1 s + \frac{1}{s^2+1}
	\end{cases} \]
	If we now try to substitute the boundary condition on the problem we can see that we have no information for $x'(t)$ at the initial time: the idea now is to solve the problem parametrically (assign a variable, in this case $A$, to the initial condition $x'(0)$ ) and solve the problem this way. At the and we have to find the constant value $A$ such that $x(1) = 1$. So by substituting we get
	\[ \begin{cases}
		s^2 \hat x(s) - A + s\hat y(s) +1 = \frac 1 {s^2+1} \\
		s \hat y(s) +1 + s\hat x(s) = \frac 1 s + \frac{1}{s^2+1}
	\end{cases} \]
	To simplify the calculation of the algebraic solution of the functions $\hat x,\hat y$ we rewrite the system in matrix form that, by inversion, can determine the solutions (by using the Kramer method):
	\[ \begin{bmatrix}
		s^2 & s  \\ s & s
	\end{bmatrix} \begin{pmatrix}
		\hat x(s) \\ \hat y(s)
	\end{pmatrix} = \begin{pmatrix}
		\frac{1}{s^2+1} + A - 1 \\ \frac 1 s + \frac{1}{s^2+1} - 1	
	\end{pmatrix} \]
	\[ \Rightarrow \qquad \hat x(s) = \dfrac{\det\begin{bmatrix}
		\frac{1}{s^2+1} + A - 1 & s  \\ \frac 1 s + \frac{1}{s^2+1} - 1 & s
	\end{bmatrix} }{\det \begin{bmatrix}
		s^2 & s  \\ s & s
	\end{bmatrix}} = \frac{sA - 1}{s^2(s-1)} \]
	Having the boundary condition set on $x(t)$, our focus is to invert only the related transform $\hat x(s)$ (because, for the sake of the problem, it's not relevant determining $y(t)$ ); by doing the parametric partial fraction expansion we have
	\[ \frac{sA - 1}{s^2(s-1)} = \frac{c_1}{s-1} + \frac{c_2}{s} + \frac{c_3}{s^2}  \]
	where, by solving the linear system, the coefficients are equals to $c_1 = A-1$, $c_2 = 1-A$ and $c_3 = 1$. At this point we have everything in order to compute the inverted function of $\hat x(s)$:
	\[ x(t) = \antilaplace{\hat x(s)}(t) = (A-1)e^t + (1-A) + t \]
	The last thing to do now is to evaluate the function $x(t)$ at the time $t=1$ and equal the result to $1$ in order to determine the coefficient $A$ related to the initial first derivative of $x$:
	\[ x(1) = (A-1) e + (1-A) + 1 = 1 \qquad \Rightarrow \quad A = 1 \]
	
	
	